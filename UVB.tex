  \documentclass[12pt,a4paper]{article}\usepackage[]{graphicx}\usepackage[]{color}
\usepackage{bm}
\usepackage{verbatim}
%% maxwidth is the original width if it is less than linewidth

%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb
\hbadness=99999

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\usepackage{alltt}%
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)

%2multibyte Version: 5.50.0.2960 CodePage: 1252
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[centertags]{amsmath}
\usepackage{graphicx}%
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[font=small,skip=5pt]{caption}
\usepackage{amsmath}
\usepackage{subfig}
\usepackage{amsthm}
\usepackage[]{algorithm2e}
%\usepackage{tikz}
%\usetikzlibrary{bayesnet}
\usepackage{url}
\usepackage{ulem}
\usepackage{afterpage}
\setcounter{MaxMatrixCols}{30}

\def\app#1#2{%
  \mathrel{%
    \setbox0=\hbox{$#1\sim$}%
    \setbox2=\hbox{%
      \rlap{\hbox{$#1\propto$}}%
      \lower1.1\ht0\box0%
    }%i
    \raise0.25\ht2\box2%
  }%
}
\def\approxprop{\mathpalette\app\relax}

\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0.08in}
\setlength{\evensidemargin}{0.08in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength\parindent{0pt}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

\title{Updating Variational Bayes for Online Inference of Time Series Data}
\author{Nathaniel Tomasetti 
\and Catherine Forbes
\and Anastasios Panagiotelis}

\begin{document}
\maketitle



\section{Introduction}
\label{sec:intro}

\begin{itemize}
\item Many time-series have data observed in an online setting, where additional information becomes available regularly
\item We extend variational bayes, a popular technique for approximate bayesian inference, to online data by the bayesian updating recurrence.
\item Existing online VB all assumed that data is iid so we can subsample in minibatches to form gradient estimates.
\item This allows online learning with time dependence in the data
\item We make it faster in IS-UVB and show a speed / performance trade-off
\item The approximation accuracy and runtime of VB, UVB, and IS-UVB are compared to `exact' MCMC based inference are compared for several simulated datasets
\item We show the marginal posteriors for the eight schools problem of \cite{Gelman2014} (BDA 3)
\end{itemize}
The paper is arranged as follows
\begin{itemize}
\item Section 2: Overview of Variational Bayes
\item Section 3: The proposed extension to Updating Variational Bayes
\item Section 4: An importance sampler to trade accuracy and runtime
\item Section 5: A few simulated data studies
\item Section 6: The schools example
\item Section 7: Discussion
\end{itemize}

\section{Background to Variational Bayes}
\label{sec:background}
{\color{blue} Before introducing our novel algorithms for updating Variational Bayes (VB) estimates in an online setting, we briefly cover some elementary concepts in the Variational Bayes literature.  A more thorough review of Variational Bayes can be found in \cite{Blei2017} and references therein.  
\\

The usual target of Bayesian inference is the posterior
\begin{equation}
\label{posterior}
p({\bm\theta} | {\bm y}_{1:T}) = \frac{p({\bm y}_{1:T} | {\bm\theta})p({\bm\theta})}{\int_{\bm \theta}p({\bm y}_{1:T} | {\bm\theta})p({\bm\theta})d{\bm\theta}}\,,
\end{equation}
where ${\bm\theta}$ is a vector of parameters and ${\bm y}_{1:T}$ denote data observed from time $1$ to $T$.  Although algorithms exist to produce a sample from this posterior on which Monte Carlo inference can be carried out, these can be computationally intensive.  As an alternative, variational Bayes' aims to approximate this posterior distribution with a parametric density $q_{\bm \lambda}({\bm\theta} |{\bm y}_{1:T})$.  Here ${\bm \lambda}$ is a vector of auxiliary parameters associated with the approximation. The presence of ${\bm y}_{1:T}$ in the notation is used to make it clear that $q$ approximates the posterior and in some instances ${\bm\lambda}$ can even depend on ${\bm y}_{1:T}$. 
\\

In VB, ${\bm \lambda}$ is chosen to minimise some error function, typically the Kullback-Leibler (KL) divergence  \citep{Kullback1951} from $q_{\bm \lambda}({\bm \theta} |{\bm y}_{1:T})$ to $p({\bm\theta} |{\bm y}_{1:T})$.  The KL divergence denoted $KL[q_{\bm \lambda}({\bm\theta} |{\bm y}_{1:T})\hspace{.1cm}||\hspace{.1cm}p({\bm\theta} |{\bm y}_{1:T})]$ is given by
\begin{equation}
\label{KL-def}
KL[q_{\bm\lambda}({\bm\theta} |{\bm y}_{1:T})\hspace{.1cm}||\hspace{.1cm}p({\bm\theta} |{\bm y}_{1:T})] = E_q \left[ \log(q_{\bm \lambda}({\bm \theta} |{\bm y}_{1:T})) - \log(p({\bm \theta} |{\bm y}_{1:T})) \right]\,.
\end{equation}

In practice, minimising the KL divergence is complicated for a number of reasons.  First, $p({\bm\theta} | {\bm y}_{1:T})$ is often only known up to proportionality due to the difficulty of evaluating the integral of the denominator in (\ref{posterior}).  This can be circumvented by solving a problem that is equivalent to minimising the KL divergence, namely maximising the evidence lower bound (ELBO).  The ELBO is given by
\begin{equation}
\label{ELBO}
\mathcal{L}(q, {\bm\lambda}) = E_q \left[\log(p({\bm \theta}, {\bm y}_{1:T})) - \log(q_{\bm\lambda}({\bm\theta} | {\bm y}_{1:T}))\right],
\end{equation} 
A second difficulty that arises in VB is that the expectation in (\ref{ELBO}) cannot be evaluated analytically.  For this reason, maximisation of the ELBO is achieved via stochastic gradient ascent (SGA).  After initalising ${\bm \lambda}$ at an appropriate value, SGA applies updates of the form
\begin{equation}
\label{gradientAscent}
{\bm\lambda}^{(m+1)} = {\bm\lambda}^{(m)} + \rho^{(m)} \widehat{\frac{\partial\mathcal{L}(q, {\bm\lambda})}{\partial {\bm\lambda}}} \bigg\rvert_{{\bm\lambda} = {\bm\lambda}^{(m)}}\,
\end{equation}
until the change from $\mathcal{L}(q, {\bm\lambda}^{(m)})$ to $\mathcal{L}(q, {\bm \lambda}^{(m+1)})$ falls below some pre-specified threshold \citep{Hoffman2013}.  The expectation in the gradient term in Equation~\ref{gradientAscent} is estimated by Monte Carlo. A popular choice is the score gradient estimator of \citet{Ranganath2014} given by
\begin{equation}
\label{scoreDeriv}
\widehat{\frac{\partial\mathcal{L}(q, {\bm\lambda})}{\partial {\bm\lambda}}}_{SC} = \sum_{j = 1}^J \frac{\partial \log(q_{\bm \lambda}({\bm\theta}^{(j)} | {\bm y}_{1:T}))}{\partial {\bm\lambda}} \left(\log(p({\bm\theta}^{(j)}, {\bm y}_{1:T})) - \log(q_{\bm\lambda}({\bm\theta}^{(j)} | {\bm y}_{1:T})) \right),
\end{equation}
and where ${\bm\theta}^{(j)}$ are drawn from the approximating density $q_{\bm\lambda}({\bm\theta} | {\bm y}_{1:T})$.  This procedure is guaranteed to converge to a local maximum \citep{Robbins1951} if the sequence $\rho^{(m)}, m = 1, \dots, \infty$, satisfies
\begin{align}
&\sum_{m=1}^{\infty} \rho^{(m)} =  \infty \\
&\sum_{m=1}^{\infty} (\rho^{(m)})^2 <  \infty.
\end{align}
\\
}
\begin{comment}
\section{Bayesian Inference}
\label{sec:Inference}


Let $y_{1:T}$ be a sequence of observed data up to some time $T$, associated with a model conditioned on some unknown parameter vector $\theta$ given by
\begin{equation}
\label{likelihood}
p(y_{1:T} | \theta) = p(y_1) \prod_{t=2}^{T} p(y_t | y_{1:t-1}, \theta).
\end{equation}
When the model is augmented with some prior distribution $p(\theta)$, the posterior distribution at time $T$ is given by
\begin{equation}
\label{posterior}
p(\theta | y_{1:T}) = \frac{p(y_{1:T} | \theta)p(\theta)}{\int_{\theta}p(y_{1:T} | \theta)p(\theta)d\theta}.
\end{equation}
\\

For many models the integral in (\ref{posterior}) does not admit an analytical solution, and numeric integration may become computationally infeasable if $\theta$ is of a sufficiently high dimension. One method to infer the posterior distribution, that is popular for its computational speed, is Variational Bayes (VB, see \citet{Blei2017} for a recent review). VB replaces the posterior distribution with a parametric approximation, denoted by $q_{\lambda}(\theta |y_{1:T})$, where $\lambda$ is a vector of auxiliary parameters associated with the approximation that may depend on the observations $y_{1:T}$.

\subsection{Variational Bayes}
\label{subsec:VB}

VB posits a family of parametric approximating distributions $q_{\lambda}(\theta |y_{1:T})$, parameterised by an auxiliary vector $\lambda$, that shares the same support as the true posterior distribution $p(\theta |y_{1:T})$. Note that $q_{\lambda}(\theta |y_{1:T})$ does not neccesarily depend on $y_{1:T}$, but this notation is used to make it clear that this distribution is an approximation for $p(\theta |y_{1:T})$. A member of the approximating family is chosen to minimise some error function, typically the Kullback-Leibler (KL) divergence from $q_{\lambda}(\theta |y_{1:T})$ to $p(\theta |y_{1:T})$, given by $KL[q_{\lambda}(\theta |y_{1:T})\hspace{.1cm}||\hspace{.1cm}p(\theta |y_{1:T})]$ \citep{Kullback1951}. The KL divergence is defined by
\begin{equation}
\label{KL-def}
KL[q_{\lambda}(\theta |y_{1:T})\hspace{.1cm}||\hspace{.1cm}p(\theta |y_{1:T})] = E_q \left[ \log(q_{\lambda}(\theta |y_{1:T})) - \log(p(\theta |y_{1:T})) \right],
\end{equation}
and is a non-negative, asymmetric measure of the discrepancy between $p(\theta |y_{1:T})$ and $q_{\lambda}(\theta | y_{1:T})$  that will be equal to zero if and only if $p(\theta | y_{1:T}) = q_{\lambda}(\theta | y_{1:T})$ almost everywhere \citep{Bishop2006}.
\\

Typically the expectation in (\ref{KL-def}) is intractable, and Monte-Carlo estimates are compuationally infeasible due to the inclusion of the term $p(\theta | y_{1:T})$, which is only known up to proportionality. Instead VB uses the Evidence Lower Bound (ELBO), denoted by $\mathcal{L}(q, \lambda)$, as an error function where
\begin{equation}
\label{ELBO}
\mathcal{L}(q, \lambda) = E_q \left[\log(p(\theta, y_{1:T})) - \log(q_{\lambda}(\theta | y_{1:T}))\right],
\end{equation}
which is evaluated with Monte-Carlo estimates
\begin{equation}
\label{ELBO-MC}
\mathcal{L}(q, \lambda) \approx \frac{1}{M} \sum_{j=1}^M \left(\log(p(\theta_{j}, y_{1:T})) - \log(q_{\lambda}(\theta_{j} | y_{1:T})) \right)
\end{equation}
where $\theta_{j} \sim q_{\lambda}(\theta | y_{1:T})$. The ELBO is equal to the negative KL divergence plus a constant, and hence maximising (\ref{ELBO}) with respect to $q_{\lambda}(\theta | y_{1:T})$ is equivalent to minimising (\ref{KL-def}).

\subsection{Stochastic Gradient Ascent}
\label{subsec:SGA}
For exponential family likelihood models, $p$, with a factorisable approximation, $q$, the characteristics of the surface of the ELBO can be exploited for optimisation in what is known as Mean Field Variational Bayes \citep{Jordan1999, Ghahramani2000a, Wainwright2008}, but for more general distributions the surface of the ELBO and its stochastic estimate are unknown. In this general case, maximisation proceeds by optimising only the auxiliary parameters $\lambda$ for a fixed distribution family $q$ with stochastic gradient ascent.
\\

SGA repeatedly takes Monte-Carlo estimates of the gradient of the ELBO with respect to $\lambda$, $\partial\mathcal{L}(q, \lambda) / \partial \lambda$, as $\widehat{\partial\mathcal{L}(q, \lambda) / \partial \lambda}$ and applies updates of the form
\begin{equation}
\label{gradientAscent}
\lambda^{(m+1)} = \lambda^{(m)} + \rho^{(m)} \widehat{\frac{\partial\mathcal{L}(q, \lambda)}{\partial \lambda}} \bigg\rvert_{\lambda = \lambda^{(m)}}
\end{equation}
until the change from $\mathcal{L}(q, \lambda^{(m)})$ to $\mathcal{L}(q, \lambda^{(m+1)})$ falls below some pre-specified threshold \citep{Hoffman2013}. Intuitively, individual elements of $\lambda$ will increase if the estimate of the slope of $\mathcal{L}(q, \lambda^{(m)})$ is positive at the current point, and will decrease if that estimate is negative, until each element of $\lambda$ reaches a point where the slope is zero. This procedure is guaranteed to converge to a local maximum \citep{Robbins1951} if the sequence $\rho^{(m)}, m = 1, \dots, \infty$, satisfies
\begin{align}
&\sum_{m=1}^{\infty} \rho^{(m)} =  \infty \\
&\sum_{m=1}^{\infty} (\rho^{(m)})^2 <  \infty.
\end{align}
\\

The score gradient estimator of \citet{Ranganath2014} is utilised in this paper, estimating the gradient of the ELBO by
\begin{equation}
\label{scoreDeriv}
\widehat{\frac{\partial\mathcal{L}(q, \lambda)}{\partial \lambda}}_{SC} = \sum_{j = 1}^M \frac{\partial \log(q_{\lambda}(\theta_{j} | y_{1:T}))}{\partial \lambda} \left(\log(p(\theta_{j}, y_{1:T})) - \log(q_{\lambda}(\theta_{j} | y_{1:T})) \right),
\end{equation}
where $\theta_{j} \sim q_{\lambda}(\theta | y_{1:T})$.

\subsection{Online Mean Field Variational Bayes}
\label{subsec:OnlineMFVB}

Mean Field Variational Bayes applied conjugate-exponential family model results in a set of mean field equations that can be written in the form of 
\begin{equation}
\lambda = f(\beta, y_{1:T}) \label{OnlineMFVB:OfflineEq}
\end{equation}
where $\beta$ denotes the prior hyperparameters. 
\\

Let $\lambda_1$ denote the MFVB optimal auxiliary parameters of $q_{\lambda_1}(\theta | y_{1:T_1})$, an approximating distribution chosen to minimise the KL divergence to the posterior after observation of $y_{1:T_1}$, obtained via the application of (\ref{OnlineMFVB:OfflineEq}),
\begin{equation}
\lambda_1 = f(\beta, y_{1:T_1}) \label{OnlineMFVB:OnlineEq1}
\end{equation}

The online MFVB approach first advocated by \cite{Ghahramani2000b} is to treat $\lambda_n$ as an updated form of the prior hyperparameters and re-apply the meanfield equations as
\begin{align}
\lambda_{n+1} = f(\lambda_n, y_{T_n+1:T_{n+1}}) \label{OnlineMFVB:OnlineEq2}
\end{align}

This results in a sequence of MFVB distributions $q_{\lambda_1}(\theta | y_{1:T_1}), q_{\lambda_2}(\theta | y_{1:T_2}), \ldots$ to approximate each of the sequence of posteriors $p(\theta | y_{1:T_1}), p(\theta | y_{1:T_2}), \ldots$.
\\ 

See \cite{Smidl2004}, \cite{Hoffman2010}, \cite{Wang2011}, \cite{Broderick2013}, and \cite{Kabisa2016} for examples of applications of Online MFVB.

\subsection{Online Stochastic Variational Bayes}
\label{subsec:OnlineSVB}

The stochastic gradient ascent approach utilised by SVB, where $\partial\mathcal{L}(q, \lambda) / \partial \lambda$ is estimated by $\widehat{\partial\mathcal{L}(q, \lambda) / \partial \lambda}$, will converge to a local maximum for any unbiased gradient estimator (\cite{Robbins1951}, \cite{Bottou1999}, \cite{Bottou2008}).
\\

Online SVB does not provide an approximate distribution at each of a sequence of time points $T_1, T_2, \ldots$, but allows for SVB inference to be substantially sped up by calculating the likelihood of only a subset of the entire dataset each iteration (\cite{Hoffman2013}, \cite{Titsias2014}).
\\

If the data $y_{1:T}$ is conditionally independent given $\theta$, the log-likelihood decomposes into the sum
\begin{equation}
\label{OnlineSVB:likelihood}
\log(p(y_{1:T} | \theta)) = \sum_{t=1}^T \log(p(y_t | \theta)).
\end{equation}

By subsetting $y_{1:T}$ into length $N$ `minibatches', where $N < T$, Online SVB replaces the log-likelihood in the gradient estimates with an unbiased estimator
\begin{equation}
\label{OnlineSVB:likelihoodEstim}
\widehat{\log(p(y_{1:T} | \theta))} = \frac{T}{N} \sum_{t=1}^N \log(p(y_t | \theta))
\end{equation}
where the sum in (\ref{OnlineSVB:likelihoodEstim}) is only over components of $y_{1:T}$ that are in the current minibatch. As this is unbiased, the gradients $\widehat{\partial\mathcal{L}(q, \lambda) / \partial \lambda}$ retain their unbiased properties.
\\

Each iteration of the gradient ascent algorithm should use a different subset of the data, so that sequential iterations cycle through the entire dataset. 
\\
\end{comment}
\section{Updating Variational Bayes}
\label{sec:UVB}

{\color{blue} We now introduce a novel algorithm for updating Variational Bayes' to settings where data is regularly being observed in an online setting.  Let $T_1, T_2, \ldots$ be a sequence of time points, from which a sequence of posterior distributions $p({\bm\theta} | {\bm y}_{1:T_1}), p({\bm\theta} | {\bm y}_{1:T_2}), \ldots$, may be desired.
\\

This is illustrated in Figure \ref{fig:updatetimeUpdate} for data taken from a vehicle in the Next Generation Simulation (NGSIM) dataset provided by the US Federal Highway Administration (FHWA), travelling towards the right. In the left panel, the vehicle has been observed up until time period $T_{n}$ resulting in data $y_{1:T_{n}}$ which can be incorporated into the posterior distribution $p(\theta | y_{1:T_{n}})$. In the right panel, after observation up until time $T_{n+1}$, an additional $T_{n+1} - T_{n}$ data points are available, and posterior inference should be updated and based on $p({\bm\theta} | {\bm y}_{1:T_{n+1}})$.

\begin{figure}[htbp]
\centering
\includegraphics[width = 0.95\textwidth, height = 0.15\textheight]{figures/carUpdate}
\caption{Left: The path of a vehicle in the NGSIM dataset that has been observed for a time period equal to $T_{n}$, where the direction of travel is from the left to the right. Right: The same vehicle at a later time $T_{n+1}$, with the extra $T_{n+1} - T_{n}$ observations denoted by the dashed line. Posterior inference about this vehicle could be updated from time $T_{1}$ to time $T_{2}$ by the inclusion of this additional information.}
\label{fig:updatetimeUpdate}
\end{figure}
The usual application of Bayes' rule at time $T_2$ involves a likelihood made up of $T_2$ terms.  However if $p({\bm\theta} | {\bm y}_{1:T_{n}})$ is available the posterior can be updated by 
\begin{equation}
\label{update:updatePost}
p({\bm\theta} | {\bm y}_{1:T_{n+1}}) \propto p({\bm y}_{T_{n}+1:T_{n+1}} | {\bm\theta}, {\bm y}_{1:T_{n}})p({\bm\theta} | {\bm y}_{1:T_{n}}),
\end{equation}
where the first term on the right hand side of (\ref{update:updatePost}) only involves $T_2-T_1$ rather than $T_2$ terms.

In cases where evaluation of the posterior is computationally demanding we propose an Updating Variational Bayes (UVB) algorithm summarised as Algorithm \ref{alg:UVB}.  The algorithm is initialised by forming the following variational approximation at time $T_1$.
\begin{equation}
\label{UVB:Time1}
q_{\lambda_1}(\theta | y_{1:T_1}) = \arg \underset{q}{\min} \hspace{1mm} KL[q_{\lambda_1} (\theta | y_{1:T_1}) \hspace{.1cm}||\hspace{.1cm}p(\theta | y_{1:T_1})].
\end{equation}
Variational approximations at subsequent intervals are formed as
\begin{equation}
\label{UVB:TimeNp1}
q_{\lambda_{n+1}}(\theta | y_{1:T_{n+1}}) = \arg \underset{q}{\min} \hspace{1mm} KL[q_{\lambda_{n+1}} (\theta | y_{1:T_{n+1}}) \hspace{.1cm}||\hspace{.1cm}\hat{p}(\theta | y_{1:T_{n+1}})],
\end{equation}
where the objective is to minimise KL divergence from a pseudo-posterior given by
\begin{equation}
\label{UVB:pHatPosterior}
\hat{p}(\theta |  y_{1:T_{n+1}}) \propto p(y_{T_{n}+1:T_{n+1}} | \theta)q_{\lambda_{n}}(\theta | y_{1:T_{n}}).
\end{equation}

\begin{algorithm}[H]
 \SetKwInOut{Input}{Input}
 \Input{Prior, Likelihood.}
 \KwResult{Approximating distribution at $T_N$.}
 Observe $y_{1:T_1}$\;
 Use (\ref{scoreDeriv}) to choose the $\lambda_1$ that minimises $KL[q_{\lambda_1}(\theta |y_{1:T_1})\hspace{.1cm}||\hspace{.1cm}p(\theta |y_{1:T_1})]$\;
 \For{$n \mbox{ in } 2, \ldots, N-1$}{
   Observe next data $y_{T_{n}+1:T_{n+1}}$\;
   Use $q_{\lambda_{n}}(\theta | y_{1:T_{n}})$ and (\ref{scoreDeriv}) to choose the $\lambda_{n+1}$ that minimises $KL[q_{\lambda_{n+1}}(\theta |y_{1:T_{n+1}})\hspace{.1cm}||\hspace{.1cm}\hat{p}(\theta |y_{1:T_{n+1}})]$
  }
 \caption{Updating Variational Bayes}
  \label{alg:UVB}
\end{algorithm}

We note some differences between the proposed UVB algorithm and a standard Variational Bayes (hereafter SVB) implementation.  First, for equally spaced intervals $H=T_{n+1}-T_{n}$ for all $n$, the likelihood component of UVB has a computational complexity of $O(H)$ compared to $O(T_{n+1})$ for SVB.  Second, for UVB $\lambda_n$ can be used as starting values in the optimisation at step $n+1$ as long as the class of approximating distributions $q$ is the same for each update.  This ensure faster convergence of variational Bayes'.  Third, an SVB algorithm at step $T_{n+1}$ can only begin at time $T_{n+1}$ once all data has been observed.  In contrast the UVB algorithm can begin when only part of the data has been observed making it well suited to online applications.  Finally, since UVB targets the pseudo-posterior rather than the true posterior we expect SVB to be more accurate.
\\

Clearly the above discussion highlights a trade-off between computational speed and accuracy for UVB and SVB.  The nature of this trade-off will be context specific depending on the model, the frequency with which data is collected in real time and the methods used for variational approximation.  These issues are investigated in a simulated setting in Section~\ref{sec:UVBSim}.}

\begin{comment}

The discussion is extended to settings where data is regularly being observed in an online setting, resulting in a sequence of time points $T_1, T_2, \ldots$, from which a sequence of posterior distributions $p(\theta | y_{1:T_1}), p(\theta | y_{1:T_2}), \ldots$, may be desired.
\\

This is illustrated in Figure \ref{fig:updatetimeUpdate} for data taken from vehicle in the Next Generation Simulation (NGSIM) dataset provided by the US Federal Highway Administration (FHWA), travelling towards the right. In the left panel, the vehicle has been observed for a time period $T_{n}$ resulting in data $y_{1:T_{n}}$ which can be incorporated into the posterior distribution $p(\theta | y_{1:T_{n}})$. In the right panel, after observation for an additional period to time $T_{n+1}$, an additional $T_{n+1} - T_{n}$ data points are available, and posterior inference could be improved by incorporating the information contained in $y_{T_{n}+1:T_{n+1}}$ to form the posterior approximation $p(\theta | y_{1:T_{n+1}})$.

\begin{figure}[htbp]
\centering
\includegraphics[width = 0.95\textwidth, height = 0.15\textheight]{figures/carUpdate}
\caption{Left: The path of a vehicle in the NGSIM dataset that has been observed for a time period equal to $T_{n}$, where the direction of travel is from the left to the right. Right: The same vehicle at a later time $T_{n+1}$, with the extra $T_{n+1} - T_{n}$ observations denoted by the dashed line. Posterior inference about this vehicle could be updated from time $T_{1}$ to time $T_{2}$ by the inclusion of this additional information.}
\label{fig:updatetimeUpdate}
\end{figure}

From Bayes Rule, the posterior distribution at time $T_{n+1}$, $p(\theta | y_{1:T_{n+1}})$ is given by
\begin{equation}
\label{update:truePost}
p(\theta | y_{1:T_{n+1}}) \propto p(y_{1:T_{n_1}} | \theta)p(\theta).
\end{equation}
where the likelihood is given by
\begin{equation}
\label{update:likelihood}
p(y_{1:T_n} | \theta) = p(y_1) \prod_{t=2}^{T_n} p(y_t | \theta, y_{1:t-1}).
\end{equation}
the product of $T_n$ terms. If the posterior at time $T_{n}$ is available, the likelihood can be reduced to a product of only $T_{n+1} - T_{n}$ terms by applying Bayes' Rule,
\begin{equation}
\label{update:updatePost}
p(\theta | y_{1:T_{n+1}}) \propto p(y_{T_{n}+1:T_{n+1}} | \theta, y_{1:T_{n}})p(\theta | y_{1:T_{n}}),
\end{equation}
However, many Bayesian computational methods require the evaluation of $p(\theta | y_{1:T_{n}})$ at arbitrary values of $\theta$, which is computationally infeasible for many posterior distributions of interest and so the more intensive (\ref{update:truePost}) is typically applied instead.
\\

This problem is circumvented by Updating Variational Bayes (UVB), which begins with an application of VB at time $T_1$, resulting in the analytically tractable approximation to the posterior distribution,
\begin{equation}
\label{UVB:Time1}
q_{\lambda_1}(\theta | y_{1:T_1}) = \arg \underset{q}{\min} \hspace{1mm} KL[q_{\lambda_1} (\theta | y_{1:T_1}) \hspace{.1cm}||\hspace{.1cm}p(\theta | y_{1:T_1})].
\end{equation}
UVB then involves the recursive use of $q_{\lambda_n}(\theta | y_{1:T_n})$ at each time $T_n$ as a substitute for the true posterior in the Bayesian update equation (\ref{update:updatePost}) to form an alternative pseudo-posterior update,
\begin{equation}
\label{UVB:pHatPosterior}
\hat{p}(\theta |  y_{1:T_{n+1}}) \propto p(y_{T_{n}+1:T_{n+1}} | \theta)q_{\lambda_{n}}.(\theta | y_{1:T_{n}}).
\end{equation}
As the right hand side of this update is analytically tractable, standard ELBO gradient optimisation methods discussed in Section \ref{subsec:SGA} may be applied to find  
\begin{equation}
\label{UVB:TimeNp1}
q_{\lambda_{n+1}}(\theta | y_{1:T_{n+1}}) = \arg \underset{q}{\min} \hspace{1mm} KL[q_{\lambda_{n+1}} (\theta | y_{1:T_{n+1}}) \hspace{.1cm}||\hspace{.1cm}\hat{p}(\theta | y_{1:T_{n+1}})].
\end{equation}
Minimising the KL divergence in this way is not equivalent to standard VB which minimises $KL[q_{\lambda_{n+1}} (\theta | y_{1:T_{n+1}}) \hspace{.1cm}||\hspace{.1cm}p(\theta | y_{1:T_{n+1}})]$ unless $q_{\lambda_{n}}(\theta |  y_{1:T_{n}}) = p(\theta |  y_{1:T_{n}})$ almost everywhere. This is only possible if the posterior is a member of the family $q$. When this is not the case UVB introduces an additional approximation error to Variational Bayes inference which depends largely on the ability of the family $q$ chosen to adequately approximate the true posterior distribution.
\\

If the updates are equally spaced such that $T_{n+1} - T_{n} = H$ for all $n$, then the computational complexity of UVB as $n$ increases is constant at $O(H)$. In contrast, the computational complexity of SVB is $O(T_{n})$ as is requires evaluation of the complete data likelihood, $p(y_{1:T_{n}} | \theta)$, at each $T_{n}$. Choosing the form of the approximating distribution to match the prior distribution allows UVB to be easily implemented, as the prior hyper-parameters are simply replaced with the optimised $\lambda_n$ values after each update.
\\

A summary of the UVB algorithm is given by Algorithm \ref{alg:UVB}.

\begin{algorithm}[H]
\SetKwInOut{Input}{Input}
\Input{Prior, Likelihood.}
\KwResult{Approximating distribution at $T_N$.}
Observe $y_{1:T_1}$\;
Use (\ref{scoreDeriv}) to choose the $\lambda_1$ that minimises $KL[q_{\lambda_1}(\theta |y_{1:T_1})\hspace{.1cm}||\hspace{.1cm}p(\theta |y_{1:T_1})]$\;
\For{$n \mbox{ in } 2, \ldots, N-1$}{
Observe next data $y_{T_{n}+1:T_{n+1}}$\;
Use $q_{\lambda_{n}}(\theta | y_{1:T_{n}})$ and (\ref{scoreDeriv}) to choose the $\lambda_{n+1}$ that minimises $KL[q_{\lambda_{n+1}}(\theta |y_{1:T_{n+1}})\hspace{.1cm}||\hspace{.1cm}\hat{p}(\theta |y_{1:T_{n+1}})]$
}
\caption{Updating Variational Bayes}
\label{alg:UVB}
\end{algorithm}

\end{comment}

\section{Importance Sampled UVB} 
\label{sec:UVBIS}
{\color{blue}
{\em Possibly merge with previous section.  In the whole section it is a little blurred whether we use $q_{\lambda_{n}}$ and an importance density for $q_{\lambda_{n}}$ or $q_{\lambda^m}$ as an importance density for $q^{\lambda_{n}}$ or both.  The computational advantages of these I think are different, I think the former still required some likelihood calculation.  Finally I read Sakaya and Klami, they seem to make a big deal of using the reparameterisation trick in that context.  I'm not sure if we have done that here.}


Potential computational gains can be achieved in UVB by incorporating ideas from importance sampling.  To illustrate we first consider importance sampling in a standard VB implementation as introduced y \cite{Sakaya2017}. Subscripts are dropped on $\bm{y}$ for ease of exposition.  To update from step $m$ to step $m+1$ of the gradient ascent algorithm the gradient in Equation\ref{gradientAscent} is approximated by Monte Carlo as shown in Equation \ref{scoreDeriv}.  This involves simulating ${\bm \theta}^{(1)},\ldots{\bm{\theta}}^{(J)}\sim q_{\lambda^{(m)}}$ and evaluating $p({\bm y}, {\bm \theta^{(j)}})$ for all $j$.  At step $m+1$ a na\"ive approach would be to simulate a new sample from  $q_{\lambda^{(m+1)}}$.  However since $q_{\lambda^{(m)}}$ is likely to be close to $q_{\lambda^{(m+1)}}$ the former can be used as an importance density for the latter.  This ease the computational burden since the approximation using importance sampling only requires $p({\bm y}, {\bm \theta^{(j)}})$ which are already available from the previous step.

We propose extending this idea to UVB in an algorithm we call Importance Sampling Updated Variation Bayes (IS-UVB).  In this algorithm, the approximation to the pseudo-posterior $q_{{\bm\lambda}_n}$ is used as an importance density for approximating gradients in solving for $q_{\bm\lambda_{n+1}}$.

Recall the gradient of the ELBO in UVB is,
\begin{equation}
\label{UVBIS:scoreGrad}
\frac{\partial\mathcal{L}(q, {\bm\lambda}_{n+1})}{\partial {\bm \lambda}_{n+1}} =
 \int_{\bm\theta} q_{{\bm\lambda}_{n+1}}({\bm\theta} | {\bm y}_{1:T_{n+1}}) \frac{\partial \log(q_{{\bm\lambda}_{n+1}} | {\bm y}_{1:T_{n+1}})}{\partial {\bm\lambda}_{n+1}} \log \left(\frac{\hat{p}({\bm y}_{1:T_{n+1}}, {\bm\theta})}{q_{{\bm\lambda}_{n+1}}({ | {\bm y}_{1:T_{n+1}})}} \right) d{\bm\theta}.
\end{equation}
By multiplying and dividing by $q_{{\bm\lambda}_n}({\bm\theta} | {\bm y}_{1:T_n})$ this can be written as an expectation with respect to $q_{{\bm\lambda}_n}$ which is the UVB approximation to the pseudo posterior based on data up until time $T_n$:
\begin{equation}
\label{UVBIS:scoreGradIS}
\frac{\partial\mathcal{L}(q, {\bm \lambda}_{n+1})}{\partial {\bm \lambda}_{n+1}} = \int_{\theta} q_{{\bm \lambda}_{n}}({\bm \theta} | {\bm y}_{1:T_{n}})\frac{q_{{\bm\lambda}_{n+1}}({\bm\theta} | {\bm y}_{1:T_{n+1}})}{q_{{\bm\lambda}_{n}}(\bm{\theta} | {\bm y}_{1:T_{n}})} \frac{\partial \log(q_{{\bm \lambda}_{n+1}}({\bm\theta} | {\bm y}_{1:T_{n+1}}))}{\partial {\bm \lambda}_{n+1}} \log \left(\frac{\hat{p}({\bm y}_{1:T_{n+1}}, {\bm\theta})}{q_{{\bm\lambda}_{n+1}}({\bm\theta} | {\bm y}_{1:T_{n+1}})} \right) d{\bm \theta}.
\end{equation}
This can be estimated via Monte Carlo,
\begin{equation}
\label{UVBIS:scoreEstIS}
\widehat{\frac{\partial\mathcal{L}(q, {\bm \lambda}_{n+1})}{\partial {\bm\lambda}_{n+1}}}_{IS} = \frac{1}{J} \sum_{j=1}^J w({\bm\theta}^{(j)})\frac{\partial \log(q_{{\bm\lambda}_{n+1}}({\bm\theta}^{(j)} | {\bm y}_{1:T_{n+1}}))}{\partial {\bm\lambda}_{n+1}} \log \left(\frac{\hat{p}({\bm y}_{1:T_{n+1}}, {\bm \theta}^{(j)})}{q_{{\bm\lambda}_{n+1}}({\bm\theta}^{(j)} | {\bm y}_{1:T_{n+1}})} \right),
\end{equation}
where ${\bm\theta} \sim q_{{\bm\lambda}_{n}}({\bm\theta} | {\bm y}_{1:{T_n}})$ and 
\begin{equation}
w({\bm\theta}^{(i)}) = \frac{q_{{\bm\lambda}_{n+1}}({\bm\theta}^{(i)} | {\bm y}_{1:T_{n+1}})}{q_{{\bm\lambda}_{n}}({\bm\theta}^{(i)} | {\bm y}_{1:T_{n}})}.
\end{equation}

Since only ${\bm \lambda}_{n+1}$ and not the parameter draws change for each step of gradient ascent, only $\frac{\partial}{\partial {\bm\lambda}_{n+1}} \log(q_{{\bm\lambda}_{n+1}}({\bm\theta} | {\bm y}_{1:T_{n+1}}))$ and $q_{{\bm\lambda}_{n+1}}({\bm\theta}^{(i)} | {\bm y}_{1:T_{n+1}})$ need to be recalculated.
\\

In the UVB case, if the marginal information about ${\bm\theta}$ contained in data observed in the interval between $T_n$ to $T_{n+1}$ is relatively small, then the KL divergence minimising value of $\lambda_{n+1}$ will not be significantly different from $\lambda_n$ and importance sampler weight decay is avoided.
\\

The variance of this estimator is increased relative to the score gradient estimator in (\ref{scoreDeriv}) due to the presence of the importance sampling weights. This increased variance can resulting in less accurate approximations, as the algorithm stopping criteria, a sufficiently small value of $|\mathcal{L}(q, \lambda^{(m+1)}) - \mathcal{L}(q, \lambda^{(m)})|$ after iteration $m+1$ is only evaluated as a noisy estimate also using the importance sampler. As the computation per iteration is extremely small we can set $N$ to be very large to compensate, allowing the user to set the trade-off between computation time approximation accuracy to suit their requirements.
\\
{\em Not sure what $N$ is here.  My hunch is that we allow the SGA to go for more steps.}

IS-UVB is summarised in Algorithm \ref{alg:UVBIS}.
\\

\vspace{2mm}
\begin{algorithm}[H]
	\SetKwInOut{Input}{Input}
	\Input{Prior, Likelihood.}
	\KwResult{Approximating distribution at $T_N$.}
	Observe $y_{1:T_1}$\;
	Use (\ref{scoreDeriv}) to choose the $\lambda_1$ that minimises $KL[q_{\lambda_1}(\theta |y_{1:T_1})\hspace{.1cm}||\hspace{.1cm}p(\theta |y_{1:T_1})]$\;
	\For{$n \mbox{ in } 1, \ldots, N-1$}{
		Observe $y_{T_{n}+1:T_{n+1}}$\;
		Sample $\theta \sim q_{\lambda_{n}}(\theta | y_{1:T_{n}})$\; 
		Calculate $p(y_{T_{n}+1:T_{n+1}} | \theta)$ and $q_{\lambda_{n}}(\theta | y_{1:T_{n}})$ for these samples\;
		Use (\ref{UVBIS:scoreEstIS}) to choose the $\lambda_{n+1}$ that minimises $KL[q_{\lambda_{n+1}}(\theta |y_{1:T_{n+1}})\hspace{.1cm}||\hspace{.1cm}\hat{p}(\theta |y_{1:T_{n+1}})]$
	}
	\caption{Importance Sampled UVB}
	\label{alg:UVBIS}
\end{algorithm}
}

\begin{comment}

UVB has significant computational cost savings as it reduces the likelihood from a product of $T_{n+1}$ terms to a product of only $T_{n+1} - T_n$ terms. \\

The gradient ascent algorithm utilised by UVB to fit $q_{\lambda_{n+1}}(\theta | y_{1:T_{n+1}})$ maximises the ELBO by repeated application of the following steps:
\begin{enumerate}
\item At iteration $m$ draw $\theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(S)}$ from $q_{\lambda_{n+1}^{(m)}}(\theta | y_{1:T_{n+1}})$ \\
(alternatively draw $\epsilon^{(1)}, \epsilon^{(2)}, \ldots, \epsilon^{(S)}$ from $r(\epsilon)$ and transform with $\theta = f(\epsilon, \lambda^{(m)}_{n+1})$),
\item Calculate $p(y_{T_{n}+1:T_{n+1}} | \theta)$ for each $\theta = \theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(S)}$ as part of the gradient estimate $\widehat{\frac{\partial\mathcal{L}(q, \lambda_{n+1})}{\partial \lambda_{n+1}}}$,
\item Update $\lambda_{n+1}^{(m+1)} = \lambda_{n+1}^{(m)} + \rho^{(m)}\widehat{\frac{\partial\mathcal{L}(q, \lambda_{n+1})}{\partial \lambda_{n+1}}} \bigg\rvert_{\lambda_{n+1} = \lambda^{(m)}_{n+1}}$,
\item Set $m = m + 1$.
\end{enumerate}
After each iteration, $\lambda_{n+1}$ changes, and hence $\theta$ must be resamples at step one, and the likelihood must be recalculated with these new samples. 
\\

\citet{Sakaya2017} note that calulation of the data likelihood is the most computationally expensive component of the gradient estimate. They propose that the $\theta$ samples used at some iteration $m$ could be re-used for iterations $m + 1, m + 2, \ldots m + h$ to calculate the gradient as an importance sampled estimate. 
\\

As $\theta$ is constant between iterations, the likelihood does not need to be recalculated and optmisation is sped up. For some $h$ the difference between $q_{\lambda_{n+1}^{(m)}}(\theta | y_{1:T_{n+1}})$ and $q_{\lambda_{n+1}^{(m+h)}}(\theta | y_{1:T_{n+1}})$ becomes large, reducing the effective sample size of the importance sampler weights and degrading the gradient estimate. At this stage \citet{Sakaya2017} resample $\theta$ and recalculate the likelihoods.
\\

This idea is readily extendable to the UVB context to derive an importance sampled UVB (IS-UVB) gradient estimator.
\\

Recall the score ELBO gradient for UVB,
\begin{equation}
\label{UVBIS:scoreGrad}
\frac{\partial\mathcal{L}(q, \lambda_{n+1})}{\partial \lambda_{n+1}} = \int_{\theta} q_{\lambda_{n+1}}(\theta | y_{1:T_{n+1}}) \frac{\partial \log(q_{\lambda_{n+1}}(\theta | y_{1:T_{n+1}}))}{\partial \lambda_{n+1}} \log \left(\frac{\hat{p}(y_{1:T_{n+1}}, \theta)}{q_{\lambda_{n+1}}(\theta | y_{1:T_{n+1}})} \right) d\theta.
\end{equation}
By multipling and dividing this by $q_{\lambda_n}(\theta | y_{1:T_n})$ it can be written as an expectation with respect to the previous UVB distribution at time $T_n$:
\begin{equation}
\label{UVBIS:scoreGradIS}
\frac{\partial\mathcal{L}(q, \lambda_{n+1})}{\partial \lambda_{n+1}} = \int_{\theta} q_{\lambda_{n}}(\theta | y_{1:T_{n}})\frac{q_{\lambda_{n+1}}(\theta | y_{1:T_{n+1}})}{q_{\lambda_{n}}(\theta | y_{1:T_{n}})} \frac{\partial \log(q_{\lambda_{n+1}}(\theta | y_{1:T_{n+1}}))}{\partial \lambda_{n+1}} \log \left(\frac{\hat{p}(y_{1:T_{n+1}}, \theta)}{q_{\lambda_{n+1}}(\theta | y_{1:T_{n+1}})} \right) d\theta,
\end{equation}
and estimated via Monte Carlo,
\begin{equation}
\label{UVBIS:scoreEstIS}
\widehat{\frac{\partial\mathcal{L}(q, \lambda_{n+1})}{\partial \lambda_{n+1}}}_{IS} = \frac{1}{M} \sum_{i=1}^M w(\theta^{(i)})\frac{\partial \log(q_{\lambda_{n+1}}(\theta^{(i)} | y_{1:T_{n+1}}))}{\partial \lambda_{n+1}} \log \left(\frac{\hat{p}(y_{1:T_{n+1}}, \theta^{(i)})}{q_{\lambda_{n+1}}(\theta^{(i)} | y_{1:T_{n+1}})} \right),
\end{equation}
where $\theta \sim q_{\lambda_{n}}(\theta | y_{1:{T_n}})$ and 
\begin{equation}
w(\theta^{(i)}) = \frac{q_{\lambda_{n+1}}(\theta^{(i)} | y_{1:T_{n+1}})}{q_{\lambda_{n}}(\theta^{(i)} | y_{1:T_{n}})}.
\end{equation}

At each gradient ascent iteration only $\frac{\partial}{\partial \lambda_{n+1}} \log(q_{\lambda_{n+1}}(\theta | y_{1:T_{n+1}}))$ and $q_{\lambda_{n+1}}(\theta^{(i)} | y_{1:T_{n+1}})$ need to be recalculated as $\lambda_{n+1}$ updates from $\lambda_{n+1}^{(m)}$ to $\lambda_{n+1}^{(m+1)}$
\\

In the UVB case, if the marginal information about $\theta$ contained in data from $T_n$ to $T_{n+1}$ is relatively small, the converged $\lambda_{n+1}$ will not be significantly different from $\lambda_n$ and importance sampler weight decay is avoided.
\\

The variance of this estimator is increased relative to the score gradient estimator in (\ref{scoreDeriv}) due to the presence of the importance sampling weights. This increased variance can resulting in less accurate approximations, as the algorithm stopping criteria, a sufficiently small value of $|\mathcal{L}(q, \lambda^{(m+1)}) - \mathcal{L}(q, \lambda^{(m)})|$ after iteration $m+1$ is only evaluated as a noisy estimate also using the importance sampler. As the computation per iteration is extremely small we can set $N$ to be very large to compensate, allowing the user to set the trade-off between computation time approximation accuracy to suit their requirements.
\\

IS-UVB is outlined in Algorithm \ref{alg:UVBIS}.
\\

\vspace{2mm}
\begin{algorithm}[H]
 \SetKwInOut{Input}{Input}
 \Input{Prior, Likelihood.}
 \KwResult{Approximating distribution at $T_N$.}
 Observe $y_{1:T_1}$\;
 Use (\ref{scoreDeriv}) to choose the $\lambda_1$ that minimises $KL[q_{\lambda_1}(\theta |y_{1:T_1})\hspace{.1cm}||\hspace{.1cm}p(\theta |y_{1:T_1})]$\;
 \For{$n \mbox{ in } 1, \ldots, N-1$}{
    Observe $y_{T_{n}+1:T_{n+1}}$\;
   Sample $\theta \sim q_{\lambda_{n}}(\theta | y_{1:T_{n}})$\; 
   Calculate $p(y_{T_{n}+1:T_{n+1}} | \theta)$ and $q_{\lambda_{n}}(\theta | y_{1:T_{n}})$ for these samples\;
  Use (\ref{UVBIS:scoreEstIS}) to choose the $\lambda_{n+1}$ that minimises $KL[q_{\lambda_{n+1}}(\theta |y_{1:T_{n+1}})\hspace{.1cm}||\hspace{.1cm}\hat{p}(\theta |y_{1:T_{n+1}})]$
  }
 \caption{Importance Sampled UVB}
  \label{alg:UVBIS}
\end{algorithm}


\end{comment}
\iffalse
\section{Approximation Error}

We consider three approaches to posterior inference:
\begin{enumerate}
\item Posterior sampling with MCMC,
\item Approximate Posterior inference with VB,
\item Approximate Posterior inference with UVB,
\end{enumerate}

As the number of iterations increases, samples generated by MCMC converge to serially correlated samples from the true posterior distribution and we will consider MCMC as exact inference for the purpose of measuring VB and UVB approximation error in this paper.
\\

In this section we discuss several choices loss functions to measure this error. In the general case, the distance between the posterior distribution $p(\theta | y_{1:T_n})$ and approximation $q_{\lambda_n}(\theta |  y_{1:T_n})$ is of interest, however there may be cases where a model specific loss is of interest: the accuracy of an application of $\theta$ is more relevant than the accuracy of the posterior approximation itself. 


\subsection{Posterior Distance}

Each VB algorithm results in a density for $q(\theta | x)$, however we only have access to the true posterior $p(\theta | y_{1:T_n})$ through a set of MCMC samples. To measure the distance between the posterior and its variational approximation we consider the empirical first order Wasserstein distance, $W_1$, between thinned MCMC samples $\theta^{(i)}_p, i = 1, \dots, N$ and independently generated samples $\theta^{(j)}_q, j = 1, \dots, N$ from $q_{\lambda_n}(\theta |  y_{1:T_n})$.
\\

The Wasserstein distance is given by
\begin{equation}
\label{wasserstein}
W_1 = \underset{\boldsymbol{\omega} \in \Omega}{\min} \sum_{i=1}^N \sum_{j=1}^N \omega_{[ij]} d_{ij}
\end{equation}
where $d_{ij}$ denotes the euclidean distance between $\theta^{(i)}_p$ and $\theta^{(j)}_q$, while $\Omega$ denotes the set of all bijections from $\theta_p$ to $\theta_q$: each $\boldsymbol{\omega}$ is a matrix with one $1$ in each row and column and zeros in all other locations.
\\

This is also known as the Earthmover distance and is calculated with the R package `transport' \citep{transport} using the forward and reverse auction algorithm of \citet{Bertsekas1992}.

\subsection{Predictive Model Loss}

For a predictive model, where the aim is to predict the value of $y_{T_n + h}$ given observations of $y_{1:T_n}$, we consider loss functions based on the accuracy of the predictive distribution.

For example, consider the AR(p) model given by
\begin{equation}
y_t = \mu + \sum_{s=1}^p \phi_s (y_{t-s} - \mu) + e_t,
\end{equation}
where $e_t \sim N(0, \sigma^2)$, with the parameter vector
\begin{equation}
\theta = \{\sigma^2, \mu, \phi_1, \dots, \phi_p \}.
\end{equation}
\\

Given the posterior distribution $p(\theta | y_{1:T_n})$, the predictive distribution associated with $y_{T_n +h}$ is given by
\begin{equation}
\label{forecastDistIntro}
p(y_{T_n + h} | y_{1:T_n}) = \int_{\theta} p(y_{T_n + h} | y_{1:T_n}, \theta)p(\theta | y_{1:T_n})d\theta.
\end{equation}
The loss function associated with this value is given by $L(n, h)$ and is measured by the predictive logscore,
\begin{equation}
\label{loss:logscoreIntro}
L(n, h) = \log(p(y_{T_n + h} | y_{1:T_n}))
\end{equation}

\subsection{Classification Model Loss}

Many models aim to associate a class label $k_i = 1 , \dots, K$ to observations of each unit $i$. Consider the problem where $T_n$ observations are recorded for each of $M$ units from a mixture of normal distributions,
\begin{equation}
\label{mixNormalDGP}
y_{i, t} \sim \sum_{j=1}^K \pi_{j} N(\mu_j, \sigma^2_{j}).
\end{equation}
At each time period we aim to infer the posterior distribution of each mixture components parameters $\{\mu_j, \sigma_j, \pi_j | k = 1, \dots, K\}$ and predict the class labels of $y_{i, 1:T_n}$. Approximation error on the mixture components is measured by the Wasserstein Distance, while class label loss function is proportion of correct classifications.

\subsection{State-Space Model Loss}

Many models of interest can be expressed as a dynamic state-space, with
\begin{align}
y_t &\sim p(y_t | x_{t}, \theta) \label{measure} \\
x_t &\sim p(x_t | x_{t-1}, \theta) \label{transition} ,
\end{align}
where inference of $p(x_t, \theta | y_{1:t})$ is desired at each $t$. This problem is known as \textit{Bayesian filtering}, where the posterior distribution is recursively updated from $p(x_{t-1}, \theta | y_{1:t-1})$ to $p(x_t, \theta | y_{1:t})$ by
\begin{align}
p(x_t, \theta | y_{1:t-1}) &= \int p(x_t | x_{t-1}, \theta) p(x_{t-1}, \theta | y_{1:t-1})dx_{t-1} \label{marginalise} \\
p(x_t, \theta | y_{1:t}) &\propto p(y_t | x_t, \theta) p(x_t, \theta | y_{1:t-1}) \label{update}
\end{align}
for a given prior distribution $p(x_0, \theta)$. 
\\

For a limited class of models the marginalisation over $x_{t-1}$ in (\ref{marginalise}) and posterior in (\ref{update}) are both analytically tractable for all $t$, such as when both (\ref{measure}) and (\ref{transition}) are linear and Gaussian. Typically this is not the case and an approximation is required, such as the extended Kalman filter \citep{Anderson1979}, the unscented Kalman filter \citep{Wan2000}, or particle filters \citet{Arulampalam2002}. Variational Particle Filtering is introduced by \citet{Smidl2008}, in cases where a subset $x_{1, t}$ of $x_t = \{x_{1, t}, x_{2, t}\}$ can be analytically marginalised, and does not discuss simultaneous inference of the static variables $\theta$ which is allowed with UVB below.
\\

Replacing $p(x_{t-1}, \theta | y_{1:t-1})$ with the Variational Bayes approximation $q(x_{t-1} | y_{1:t-1}, \theta)q(\theta|y_{1:t-1})$ we obtain 
\begin{align}
\hat{p}(x_t, \theta | y_{1:t-1}) &= q(\theta | y_{1:t-1}) \int p(x_t | x_{t-1}, \theta) q(x_{t-1} | y_{1:t-1}, \theta)dx_{t-1} \label{marginaliseHat} \\
\hat{p}(x_t, \theta | y_{1:t}) &\propto p(y_t | x_t, \theta)\hat{p}(x_t, \theta | y_{1:t-1}). \label{updateHat}
\end{align}

If (\ref{transition}) is linear and Gaussian, and  $q(x_{t-1} | y_{1:t-1}, \theta)$ is Gaussian, (\ref{marginaliseHat}) can be marginalised analytically, allowing the Variational Bayes approximation at time $t$ as
\begin{equation}
\label{UVBfilter}
\hat{p}(x_t, \theta | y_{1:t}) \approx q(x_{t} | y_{1:t}, \theta)q(\theta|y_{1:t}).
\end{equation}
\\

Let $a_{t, \alpha}$ and $b_{t, \alpha}$ be the lower and upper bounds of the $\alpha\%$ highest probability interval for $x_t$, that is
\begin{equation}
\label{HPI}
\{a_{t, \alpha}, b_{t, \alpha}\} = \arg \underset{\{a, b\}}{\min}\mbox{ } b - a, \mbox{ where } \int_a^b p(x_t | x_{1:t-1}, \theta, y_{1:t})dx_t = \alpha.
\end{equation}
\\

We define the loss function as the $\alpha\%$ latent state coverage rate, 
\begin{equation}
\label{coverage}
L(n, \alpha) = \frac{\sum_{t=1}^{T_n} I(a_{t, \alpha} < x_t < b_{t, \alpha})}{T_n}
\end{equation}
where $I(\cdot)$ is the indicator function, which equals one if the argument is true and zero otherwise.
\fi

\section{Simulation Results}
\label{sec:UVBSim}

The approximation error for repeated UVB and Importance Sampled UVB (IS-UVB) applications is compared relative to both SVB inference and exact inference implemented by RWMH-MCMC for two simple problems: Time Series Forecasting and Mixture Model Clustering.

\subsection{Time Series Forecasting}
\label{subsec:UVBTS}

500 datasets $y_{1:500}$ are simulated according to the following AR3 model,
\begin{equation}
\label{UVB:TSAR3}
y_t = \mu + \phi_1 (y_{t-1} - \mu) + \phi_2 (y_{t-2} - \mu) + \phi_3 (y_{t-3} - \mu) + e_t
\end{equation}
where $e_t \sim N(0, \sigma^2)$. 
The parameters are $\theta = \{\log(\sigma^2), \mu, \phi_1, \phi_2, \phi_3 \}$, with the prior
\begin{equation}
\label{UVB:TSprior}
\theta \sim N(\boldsymbol{0}, 10 \mathbb{I})
\end{equation}
where $\boldsymbol{0}$ denotes the zero vector and $\mathbb{I}$ denotes the identity matrix.
\\

In each simulation, $\mu$ and each $\phi$ are drawn from a $N(0, 1)$ distribution, accepting only draws where each $\phi$ lies in the AR3 stationary region. $\sigma^{-2}$ is simulated from a $G(5, 5)$ distribution.
\\

At each time $t = 100, 101, \ldots, 300$, the MCMC posterior distribution $p(\theta | y_{1:t})$ is simulated by 15000 draws from a RWMH-MCMC algorithm, discarding the first 10000 as a burn in period.
\\

SVB, UVB, and IS-UVB approximations, $q_{VB}(\theta | y_{1:T_n})$, $q_{UVB}(\theta | y_{1:T_n})$, and $q_{IS-UVB}(\theta | y_{1:T_n})$ respectively, are fit at each time $T_n = 75 + 25n$ for $n = 1, 2, \ldots 17$. In each case, the distributional family for $q$ is chosen to be a $K = 1, 2$, or $3$ component mixture of multivariate normal distributions.
\\

SVB and UVB use the score gradient estimator calculated from $M = 25$ $\theta$ samples per iteration, while IS-UVB is ran with 100 $\theta$ samples re-used between iterations.
\\

Given the posterior, or its approximation, the forecast distribution for $y_{T_n+h}$ is given by
\begin{equation}
\label{UVB:TSforecastDist}
p(y_{T_n + h} | y_{1:T_n}) = \int_{\theta} p(y_{T_n + h} | y_{1:T_n}, \theta)p(\theta | y_{1:T_n})d\theta,
\end{equation}
which may be evaluated by $M$ samples from the posterior,
\begin{equation}
\label{UVB:TSforecastDistApprox}
\hat{p}(y_{T_n + h} | y_{1:T_n}) \approx \frac{1}{M} \sum_{m=1}^M  p(y_{T_n + h} | y_{1:T_n}, \theta^{(m)}).
\end{equation}
The loss function associated with this value is given by $L(n, h)$ and is measured by the predictive logscore,
\begin{equation}
\label{UVB:TSlogscore}
L(n, h) = \log(p(y_{T_n + h} | y_{1:T_n})).
\end{equation}

One step ahead forecast distributions are made for each inferential method: MCMC, SVB, UVB and IS-UVB. Each approximate inference technique is evaluated by its predictive logscore. The results are displayed in Figure \ref{fig:UVBAR3Timing}, where the left panel displayed the mean logscore for each method after observation up to $T_n = 100, 125, \ldots, 500$ across the 500 simulations. While each approximation has a lower logscore compared to exact inference, there is little realised loss in logscore resulting from using IS-UVB over SVB, however UVB performs better than both methods.
\\

The right panel displayed the mean runtime for a singular SVB fit with data up to $T_n$, compared to the mean runtime for UVB and IS-UVB fit to $T_1$ and updated $n-1$ times to $T_n$, relative to the time taken for the initial fit. In this scenario, the amount of data is small and the computational cost of calculating the log-likelihood is low, so UVB is slower than SVB. However, UVB benefits by being able to provide an approximating distribution from $T_1$ onwards, while SVB must wait until all data is observed at time $T_n$ to begin optimisation. IS-UVB is significantly faster than either alternative. There is no significant difference between $K = 1, 2 $ or $3$.   

\begin{figure}[htbp]
    \centering
    {{\includegraphics[width=7cm, height = 6cm]{figures/AR3ls} }}%
    \qquad
    {{\includegraphics[width=7cm, height = 6cm]{figures/AR3timing} }}%
    \caption{Left: Average predictive logscores for each inference technique. Right: Average SVB runtime for one approximation at time $T_n$ and average cumulative UVB and IS-UVB runtimes fit to $T_1$, then updated $n-1$ times to $T_n$. Runtimes are relative to the time required for the $T_1$ fit.
UVB performs better than SVB but is slower, while IS-UVB performs worse but is faster. Both UVB and IS-UVB can begin optimisation as data arrives rather than waiting for all data to be observed.}%
    \label{fig:UVBAR3Timing}%
\end{figure}

\subsection{Mixture Model Clustering}
\label{subsec:UVBMMC}

The two component mixture normal model for $i = 1, \ldots N$ and $t = 1, \ldots, T_n$ is considered, augmented each unit $i$ with an auxiliary variables $k_i$ such that
\begin{equation}
\label{UVB:MMCmixNormalDGP2}
y_{i, t} | k_i = j \sim  N(\mu_j, \sigma^2_{j}).
\end{equation}
with 
\begin{equation}
\label{UVB:MMCkPrior}
k_i | \pi \sim Bin(1, \pi).
\end{equation}

Collecting $\theta = \{\log(\sigma^2_1), \log(\sigma^2_2), \mu_1, \mu_2 \}$, the prior for $\theta$ is given by
\begin{align}
\theta \sim N(\boldsymbol{0}, 10 \mathbb{I}), \\
\pi_1 \sim Beta(\alpha, \beta). \label{UVB:MMCpiPriorMix}
\end{align}


Note that (\ref{UVB:MMCkPrior}) and (\ref{UVB:MMCpiPriorMix}) imply that 
\begin{equation}
\label{UVB:MMCkMarginalMix}
p(k_i = j) = \frac{\mathcal{B}(j + \alpha, \beta - j + 1)}{\mathcal{B}(\alpha, \beta)}
\end{equation}
where $\mathcal{B}(\cdot, \cdot)$ denotes the Beta function.
\\

Denoting $y_{i, 1:T_n} = \{y_{i, t} | t = 1, \ldots, T_n\}$ and $y_{1:N, 1:T_n} = \{y_{i, 1:T_n} | i = 1, \ldots N \}$, the posterior distribution can then be marginalised over the values of each $k_i$,
\begin{equation}
\label{UVB:MMCMarginal}
p(\theta | y_{1:N, 1:T_n}) \propto p(\theta) \prod_{i=1}^N \left( \sum_{j=1}^2 p(y_{i, 1:T_n} | \theta, k_i = j) p(k_i = j) \right)
\end{equation}
This is approximated by employing distributions $q_{\lambda}(\theta | y_{1:N, 1:T_n})$ as either a $K = 1, 2,$ or $3$ component mixture of multivariate normal distributions. 
\\

As in Section \ref{subsec:UVBTS}, SVB and UVB use the score gradient estimator calculated from $M = 25$ $\theta$ samples per iteration, while IS-UVB is ran with 100 $\theta$ samples re-used between iterations.
\\

This posterior approximation can be updated if at each time $T_{n+1}$ the marginalisation in (\ref{UVB:MMCMarginal}) can be applied as
\begin{equation}
\label{UVB:MMCUpdate}
\hat{p}(\theta | y_{1:T_{n+1}}) \propto q_{\lambda_{n}}(\theta | y_{i, 1:T_{n}}) \prod_{i=1}^N \left( \sum_{j=1}^2 p(y_{i, T_n+1:T_{n+1}} | \theta, k_i = j) p(k_i = j | y_{1:T_n}) \right)
\end{equation}
This requires the marginal posterior distributions $p(k_i | y_{i, 1:T_{n}})$, which can be  approximately evaluated by
\begin{align}
\hat{p}(k_i | y_{i, 1:T_{n}}) &= \int_{\theta} p(k_i | y_{i, 1:T_{n}}, \theta)q_{\lambda_{n}}(\theta | y_{i, 1:T_{n}}) d\theta \nonumber \\
&\propto \int_{\theta} p(y_{i, 1:T_n} | \theta, k_i) p(k_i) q_{\lambda_{n}}(\theta | y_{i, 1:T_{n}}) d\theta \nonumber \\
&\approx \frac{1}{M} \sum_{m=1}^M p(y_{i, 1:T_n} | \theta^{(m)} , k_i) p(k_i). \label{UVB:MMCpkHat}
\end{align}
where $\theta^{(m)} \sim q_{\lambda_{n}}(\theta | y_{i, 1:T_{n}})$ for $m = 1, 2, \ldots, M$. This step happens once per UVB update, so use of the entire dataset in (\ref{UVB:MMCpkHat}) does not incur a large computational cost.
\\

This distribution is also used to predict classes for each unit as
\begin{equation}
\hat{k}_i = \arg \underset{j}{\max}\mbox{ } \hat{p}(k_i = j | y_{i, 1:T_n}).
\end{equation}

The accuracy of each inferential method in this model is given by the proportion of correct class predictions at each $T_n$.
\\

500 datasets are simulated with $N = 100$ units of length $T = 100$, and the posterior distribution $p(\theta, k_1, \ldots, k_N | y_{1:N, 1:T_n})$ is inferred for each $T_n = 10n, n = 1, 2, \ldots, 10$ using each inferential method: MCMC, SVB, UVB, and IS-UVB. Data is simulated by drawing each $\mu$ from an $N(0, 0.25)$ distribution and each $\sigma^2$ from a $U(1, 2)$ distribution. Units are randomly allocated to each group with $50\%$ probability. 
\\

The results are displayed in Figure \ref{fig:UVBMMCResults}, where breaking the data into smaller pieces benefits the accuracy of the UVB and IS-UVB posteriors, and hence cluster allocation score in the left panel. This problem features a large amount of data, and the computational cost of calculating the log-likelihood over the entire sample required by SVB slows optimisation relative to UVB and IS-UVB which process smaller amounts of data, shown in the right panel. The value of $K$ does not have a significant impact.

\begin{figure}[htbp]
    \centering
    {{\includegraphics[width=7cm, height = 6cm]{figures/mixNormScore} }}%
    \qquad
    {{\includegraphics[width=7cm, height = 6cm]{figures/mixNormTiming} }}%
    \caption{Left: Average proportion of correct classifications for each inference method. Right: Average SVB runtime for one approximation at time $T_n$ and average cumulative UVB and IS-UVB runtimes fit to $T_1$, then updated $n-1$ times to $T_n$. Runtimes are relative to the time required for the $T_1$ fit.    
UVB and IS-UVB perform better than SVB and are significantly faster, as computation of the data likelihood is a large part of the gradient calculation in this scenario.}%
    \label{fig:UVBMMCResults}%
\end{figure}

\section{Eight Schools Example}

In this section we consider the `Eight Schools' problem from \citet{Gelman2014}, where data from students' results on the SAT-V, a standardised highschool test in the USA, is collected to determine the effectiveness of SAT coaching programs ran by each of eight schools. 
\\

For each school $j = 1, 2, \ldots, 8$ it is assumed that an individual student's coaching effect, $y_{i, j}, i = 1, 2, \dots n_j$ is distributed with unknown mean $\theta_j$ and known variance $\sigma^2$, which is estimated by the sample variance.
\\

Due to the Central Limit Theorem \citep{Casella2002} the per school sample means,
\begin{equation}
\label{schools:mean}
\bar{y}_j = \frac{1}{n_j} \sum_{i=1}^{n_j} y_{i, j}
\end{equation}
are asymptotically distributed according to
\begin{equation}
\label{schools:CLT}
\bar{y}_{j} \sim \mathcal{N}(\theta_j, \sigma^2_j)
\end{equation}
where
\begin{equation}
\label{schools:var}
\sigma_j^2 = \frac{\sigma^2}{n_j}.
\end{equation}
\citet{Gelman2014} apply a hierarchical model, where the data is distributed according to (\ref{schools:CLT}) and each unknown variable $\theta_j$ is distributed according to the generalised student's t distribution,
\begin{equation}
\label{schools:hier}
\theta_j \sim t(\mu, \tau, \nu)
\end{equation}
where $\mu$ is a location parameter, $\tau$ is a scale parameter, and $\nu$ is the degrees of freedom, fixed at $\nu = 4$. Each $\theta_j$ is assumed to be conditionally independent of each other $\theta_i$ and $y_{1:n_i, i}, i \neq j$, given $\mu$ and $\tau$. 
\\

\citet{Gelman2014} also employ the hyper-prior
\begin{equation}
\label{schools:hyperprior}
p(\mu, \tau) \propto 1.
\end{equation}

Collecting $\theta^* = \{\theta_1,  \ldots, \theta_8, \mu, \log(\tau)\}$ and $\textbf{y}_{j} = \{y_{i, j} \hspace{2mm}| \hspace{2mm} i = 1,\ldots n_j\}$, the posterior distribution $p(\theta^* | \textbf{y}_{1:8})$ is given by
\begin{equation}
\label{schools:posterior}
p(\theta^* | \textbf{y}_{1:8}) \propto  p(\mu, \tau) \prod_{j=1}^8 \mathcal{N}(\bar{y}_j | \theta_j, \sigma^2_j) t(\theta_j | \tau, \mu, \nu).
\end{equation}

MCMC posterior samples are generated according to the algorithm provided by Stan, \citep{RStanGettingStarted}, and is compared to each of SVB, UVB, and IS-UVB.
\\

Each variational algorithm follows \citet{Kucukelbir2017} and uses a multivariate normal distribution $q(\theta^* | \textbf{y}_{1:8})$. In the case of UVB and IS-UVB, each update increases the number of schools included by one, using the posterior decomposition
\begin{equation}
\label{schools:update}
p(\theta_1:m, \mu, \tau | \textbf{y}_{1:m}) \propto p(\textbf{y}_m | \theta_m)p(\theta_m | \mu, \tau)p(\theta_{1:m-1}, \mu, \tau | \textbf{y}_{1:m-1})
\end{equation}
for $m = 2, \ldots 8$.
\\

\begin{figure}[htbp]
\centering
\includegraphics[width = 0.9\textwidth]{figures/schools}
\caption{Marginal posterior distributions for each variable using each of MCMC, SVB, UVB and IS-UVB. No variational approach is clearly better than the others, and each marginal distribution for $\tau$ is inaccurate, possibly due to the impliclit lognormal approximation begin unsuitable.}
\label{fig:schools}
\end{figure}

The resulting marginal posterior distributions are displayed in Figure \ref{fig:schools}. Each variational approach captures the MCMC samples for $\mu$ well, $\tau$ poorly, and have varying degrees of accuracy for each $\theta_j$. Neither updating method appears to have introduced any discernable error relative to SVB. 
\\

It is likely that the normal approximation for $\log(\tau)$, and implicit log-normal distribution for $\tau$, is not suitable for this distribution.

\section{Discussion}
\label{sec:UVBSummary}

In this paper Updating Variational Bayes (UVB) was introduced, a framework that allows online variational inference of data. Online varitional inference has attracted a large amount of attention in the context of Mean Field Variational Bayes (MFVB), see eg. \cite{Hoffman2010} or \cite{Broderick2013}. 
\\

UVB differs from these as it is derived from SVB rather than MFVB, and thus can be applied to a larger class of models and approximating distributional families.
\\

The approach used by UVB is to take the previous VB approximation at time $T_{n-1}$ as a prior distribution to the posterior at time $T_{n}$, which can be significantly faster than re-running SVB using all available data as it reduces the likelihood calculations to contain $T_{n} - T_{n-1}$ terms rather than $T_{n}$ terms in SVB. 
\\

Online SVB based inference, such as in \citet{Paisley2012}, work by taking sub-samples of the data series, known as `mini-batches', of size $N$ each iteration. This then reduces the likelihood calculation to contain only $N$ terms. Once scaled by a factor of $T_{n} / N$,  this gradient estimate is unbiased if the data is statistically independent. However, time series data commonly violates the independence assumption and mini-batch approaches to optimisation fail. UVB does not require independence and hence can be applied to time-series data.  
\\

UVB was extended into IS-UVB in Section \ref{sec:UVBIS}, which further reduces the computation time of each update by re-using the $\theta$ samples and likelihood calculations between iterations.
\\

Two simulation studies are presented in Section \ref{sec:UVBSim}, which demonstrates that UVB outperforms SVB in each problem, in terms of forecasting logscore or cluster allocation accuracy. In the time-series forecasting application, where the total amount of data is relatively small, fitting SVB to all observations up to time $T_1$ and the updating this to $T_2, T_3, \ldots, T_n$ via UVB has a runtime simialr to fitting SVB directly to time $T_n$. In the mixture model clustering application gradient calculations are dominated by the larger amount of data, and a time $T_1$ SVB fit followed by UVB updates is significantly faster than a $T_n$ SVB fit at all time-points.
\\

IS-UVB is shown to be faster than UVB in both problems, with performance in-between SVB and UVB. The IS-UVB gradient estimator has a larger variance than the UVB estimator, which may lead to the stopping criteria at iteration $m$, $|\mathcal{L}(q, \lambda^{(m)} - \mathcal{L}(q, \lambda^{(m-1)})| < \epsilon$ for some small $\epsilon$ , being achieved early due to the noise in the estimates of $\mathcal{L}(q, \lambda^{(m)})$ and $\mathcal{L}(q, \lambda^{(m-1)})$. This may then degrade the quality of the resulting approximate distribution. In practice $M$, the number of $\theta$ samples used in the gradient estimate, may be increased to improve the fit at the cost of computation time.
\\

Finally, SVB, UVB, and IS-UVB are applied to the Eight Schools problem of \citet{Gelman2014}, where we show that the marginal distributions of each parameter are similar under each variational algorithm.
\\

UVB is highly suitable to time-series data under the following constraints:
\begin{enumerate}
\item Up to date inference on $\theta$ is required at all times to make an action such as forecasting,
\item Data arrives rapidly so that the computation of the infernece on $\theta$ has time constraints.
\end{enumerate}
Future work involves the application of UVB to problems that meet this constraints.

\bibliographystyle{asa}
\bibliography{references}





\end{document}