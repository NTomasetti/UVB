  \documentclass[12pt,a4paper]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\usepackage{alltt}%
\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)

%2multibyte Version: 5.50.0.2960 CodePage: 1252
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[centertags]{amsmath}
\usepackage{graphicx}%
\usepackage{natbib}
\usepackage{color}
\usepackage[dvipsnames,svgnames*]{xcolor}
\usepackage{array}
\usepackage[hidelinks]{hyperref}
\usepackage[font=small,skip=5pt]{caption}
\usepackage{amsmath}
\usepackage{subfig}
\usepackage{amsthm}
\usepackage[]{algorithm2e}
%\usepackage{tikz}
%\usetikzlibrary{bayesnet}
\usepackage{url}
\usepackage{ulem}
\usepackage{afterpage}
\setcounter{MaxMatrixCols}{30}

\def\app#1#2{%
  \mathrel{%
    \setbox0=\hbox{$#1\sim$}%
    \setbox2=\hbox{%
      \rlap{\hbox{$#1\propto$}}%
      \lower1.1\ht0\box0%
    }%i
    \raise0.25\ht2\box2%
  }%
}
\def\approxprop{\mathpalette\app\relax}

\setlength{\topmargin}{0in}
\setlength{\oddsidemargin}{0.08in}
\setlength{\evensidemargin}{0.08in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\setlength\parindent{0pt}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

\title{Updating Variational Bayes for Online Inference and Forecasting of Smart Meter Data}
\author{Nathaniel Tomasetti 
\and Catherine Forbes
\and Anastasios Panagiotelis}

\begin{document}
\maketitle



\section{Introduction}
\label{sec:intro}

\begin{itemize}
\item Many time-series have data observed in an online setting, where additional information becomes available regularly
\item We extend variational bayes, a popular technique for approximate bayesian inference, to online data by the bayesian updating recurrence.
\item The approximation accuracy and runtime of VB, UVB and exact MCMC based inference are compared for several simulated datasets
\item UVB is applied to online inference of smart meter data, where the high dimensionality and frequency of observations prohibits the use of MCMC.
\item This allows us to build a model that incorporates household heterogeneity for probabilistic forecasts of short term electricity demand.
\item The results improve on a homogeneous model by X\%.
\end{itemize}
The paper is arrange as follows
\begin{enumerate}
\item Overview of Variational Bayes
\item The proposed extension to Updating Variational Bayes
\item An importance sampler to trade accuracy and runtime
\item A few simulated data studies
\item The electricity forecasting
\end{enumerate}

Aims:
\begin{itemize}
\item VB Updating to infer a posterior distribution in an online data setting
\item Computational Efficiency for intractable models
\item Explore approximation error for UVB and VB - Compare different q distributions
\item Show performance on simulated data sets
\item Apply to short term load forecasting with smart meter data
\end{itemize}

Related Literature
\begin{itemize}
\item Kalman Filters, Extended Kalman Filters, Unscented Filter
\item Particle Filters
\item Varitional Bayesian Filtering: Vermaak, Lawrence \& Perez 2003, Smidl 2006, Jin \& Mokhtarian 2007, Smidl \& Quinn 2008
\item Streaming Variational Bayes - Broderick et al. 2013
\item The entirety of the STLF literature
\end{itemize}

\section{Bayesian Inference}
\label{sec:Inference}

Let $y_{1:T}$ be a sequence of observed data up to some time $T$, associated with a model conditioned on some unknown parameter vector $\theta$ given by
\begin{equation}
\label{likelihood}
p(y_{1:T} | \theta) = p(y_1) \prod_{t=2}^{T} p(y_t | y_{1:t-1}, \theta).
\end{equation}
When the model is augmented with some prior distribution $p(\theta)$, the posterior distribution at time $T$ is given by
\begin{equation}
\label{posterior}
p(\theta | y_{1:T}) = \frac{p(y_{1:T} | \theta)p(\theta)}{\int_{\theta}p(y_{1:T} | \theta)p(\theta)d\theta}.
\end{equation}
\\

For many models the integral in (\ref{posterior}) does not admit an analytical solution, and numeric integration may become computationally infeasable if $\theta$ is of a sufficiently high dimension. One method to infer the posterior distribution, that is popular for its computational speed, is Variational Bayes (VB, see \citet{Blei2017} for a recent review). VB replaces the posterior distribution with a parametric approximation, denoted by $q_{\lambda}(\theta |y_{1:T})$, where $\lambda$ is a vector of auxiliary parameters associated with the approximation that may depend on the observations $y_{1:T}$.

\subsection{Variational Bayes}
\label{subsec:VB}

VB posits a family of parametric approximating distributions $q_{\lambda}(\theta |y_{1:T})$, parameterised by an auxiliary vector $\lambda$, that shares the same support as the true posterior distribution $p(\theta |y_{1:T})$. Note that $q_{\lambda}(\theta |y_{1:T})$ does not neccesarily depend on $y_{1:T}$, but this notation is used to make it clear that this distribution is an approximation for $p(\theta |y_{1:T})$. A member of the approximating family is chosen to minimise some error function, typically the Kullback-Leibler (KL) divergence from $q_{\lambda}(\theta |y_{1:T})$ to $p(\theta |y_{1:T})$, given by $KL[q_{\lambda}(\theta |y_{1:T})\hspace{.1cm}||\hspace{.1cm}p(\theta |y_{1:T})]$ \citep{Kullback1951}. The KL divergence is defined by
\begin{equation}
\label{KL-def}
KL[q_{\lambda}(\theta |y_{1:T})\hspace{.1cm}||\hspace{.1cm}p(\theta |y_{1:T})] = E_q \left[ \log(q_{\lambda}(\theta |y_{1:T})) - \log(p(\theta |y_{1:T})) \right],
\end{equation}
and is a non-negative, asymmetric measure of the discrepancy between $p(\theta |y_{1:T})$ and $q_{\lambda}(\theta | y_{1:T})$  that will be equal to zero if and only if $p(\theta | y_{1:T}) = q_{\lambda}(\theta | y_{1:T})$ almost everywhere \citep{Bishop2006}.
\\

Typically the expectation in (\ref{KL-def}) is intractable, and Monte-Carlo estimates are compuationally infeasible due to the inclusion of the term $p(\theta | y_{1:T})$, which is only known up to proportionality. Instead VB uses the Evidence Lower Bound (ELBO), denoted by $\mathcal{L}(q, \lambda)$, as an error function where
\begin{equation}
\label{ELBO}
\mathcal{L}(q, \lambda) = E_q \left[\log(p(\theta, y_{1:T})) - \log(q_{\lambda}(\theta | y_{1:T}))\right],
\end{equation}
which is evaluated with Monte-Carlo estimates
\begin{equation}
\label{ELBO-MC}
\mathcal{L}(q, \lambda) \approx \frac{1}{M} \sum_{j=1}^M \left(\log(p(\theta_{j}, y_{1:T})) - \log(q_{\lambda}(\theta_{j} | y_{1:T})) \right)
\end{equation}
where $\theta_{j} \sim q_{\lambda}(\theta | y_{1:T})$. The ELBO is equal to the negative KL divergence plus a constant, and hence maximising (\ref{ELBO}) with respect to $q_{\lambda}(\theta | y_{1:T})$ is equivalent to minimising (\ref{KL-def}).

\subsection{Stochastic Gradient Ascent}
\label{subsec:SGA}
For exponential family likelihood models, $p$, with a factorisable approximation, $q$, the characteristics of the surface of the ELBO can be exploited for optimisation in what is known as Mean Field Variational Bayes \citep{Jordan1999, Ghahramani2000, Wainwright2008}, but for more general distributions the surface of the ELBO and its stochastic estimate are unknown. In this general case, maximisation proceeds by optimising only the auxiliary parameters $\lambda$ for a fixed distribution family $q$ with stochastic gradient ascent.
\\

SGA repeatedly takes Monte-Carlo estimates of the gradient of the ELBO with respect to $\lambda$, $\partial\mathcal{L}(q, \lambda) / \partial \lambda$, as $\widehat{\partial\mathcal{L}(q, \lambda) / \partial \lambda}$ and applies updates of the form
\begin{equation}
\label{gradientAscent}
\lambda^{(m+1)} = \lambda^{(m)} + \rho^{(m)} \widehat{\frac{\partial\mathcal{L}(q, \lambda)}{\partial \lambda}} \bigg\rvert_{\lambda = \lambda^{(m)}}
\end{equation}
until the change from $\mathcal{L}(q, \lambda^{(m)})$ to $\mathcal{L}(q, \lambda^{(m+1)})$ falls below some pre-specified threshold \citep{Hoffman2013}. Intuitively, individual elements of $\lambda$ will increase if the estimate of the slope of $\mathcal{L}(q, \lambda^{(m)})$ is positive at the current point, and will decrease if that estimate is negative, until each element of $\lambda$ reaches a point where the slope is zero. This procedure is guaranteed to converge to a local maximum \citep{Robbins1951} if the sequence $\rho^{(m)}, m = 1, \dots, \infty$, satisfies
\begin{align}
&\sum_{m=1}^{\infty} \rho^{(m)} =  \infty \\
&\sum_{m=1}^{\infty} (\rho^{(m)})^2 <  \infty.
\end{align}
\\

The score gradient estimator of \citet{Ranganath2014} is utilised in this paper, estimating the gradient of the ELBO by
\begin{equation}
\label{scoreDeriv}
\widehat{\frac{\partial\mathcal{L}(q, \lambda)}{\partial \lambda}}_{SC} = \sum_{j = 1}^M \frac{\partial \log(q_{\lambda}(\theta_{j} | y_{1:T}))}{\partial \lambda} \left(\log(p(\theta_{j}, y_{1:T})) - \log(q_{\lambda}(\theta_{j} | y_{1:T})) \right),
\end{equation}
where $\theta_{j} \sim q_{\lambda}(\theta | y_{1:T})$.

\section{Updating Variational Bayes}
\label{sec:UVB}

The discussion is extended to settings where data is regularly being observed in an online setting, resulting in a sequence of time points $T_1, T_2, \ldots$, from which a sequence of posterior distributions $p(\theta | y_{1:T_1}), p(\theta | y_{1:T_2}), \ldots$, may be desired.
\\

This is illustrated in Figure \ref{fig:updatetimeUpdate} for data taken from vehicle in the Next Generation Simulation (NGSIM) dataset provided by the US Federal Highway Administration (FHWA), travelling towards the right. In the left panel, the vehicle has been observed for a time period $T_{n}$ resulting in data $y_{1:T_{n}}$ which can be incorporated into the posterior distribution $p(\theta | y_{1:T_{n}})$. In the right panel, after observation for an additional period to time $T_{n+1}$, an additional $T_{n+1} - T_{n}$ data points are available, and posterior inference could be improved by incorporating the information contained in $y_{T_{n}+1:T_{n+1}}$ to form the posterior approximation $p(\theta | y_{1:T_{n+1}})$.

\begin{figure}[htbp]
\centering
\includegraphics[width = 0.95\textwidth, height = 0.15\textheight]{figures/carUpdate}
\caption{Left: The path of a vehicle in the NGSIM dataset that has been observed for a time period equal to $T_{n}$, where the direction of travel is from the left to the right. Right: The same vehicle at a later time $T_{n+1}$, with the extra $T_{n+1} - T_{n}$ observations denoted by the dashed line. Posterior inference about this vehicle could be updated from time $T_{1}$ to time $T_{2}$ by the inclusion of this additional information.}
\label{fig:updatetimeUpdate}
\end{figure}

From Bayes Rule, the posterior distribution at time $T_{n+1}$, $p(\theta | y_{1:T_{n+1}})$ is given by
\begin{equation}
\label{update:truePost}
p(\theta | y_{1:T_{n+1}}) \propto p(y_{1:T_{n_1}} | \theta)p(\theta).
\end{equation}
where the likelihood is given by
\begin{equation}
\label{update:likelihood}
p(y_{1:T_n} | \theta) = p(y_1) \prod_{t=2}^{T_n} p(y_t | \theta, y_{1:t-1}).
\end{equation}
the product of $T_n$ terms. If the posterior at time $T_{n}$ is available, the likelihood can be reduced to a product of only $T_{n+1} - T_{n}$ terms by applying Bayes' Rule,
\begin{equation}
\label{update:updatePost}
p(\theta | y_{1:T_{n+1}}) \propto p(y_{T_{n}+1:T_{n+1}} | \theta, y_{1:T_{n}})p(\theta | y_{1:T_{n}}),
\end{equation}
However, many Bayesian computational methods require the evaluation of $p(\theta | y_{1:T_{n}})$ at arbitrary values of $\theta$, which is computationally infeasible for many posterior distributions of interest and so the more intensive (\ref{update:truePost}) is typically applied instead.
\\

This problem is circumvented by Updating Variational Bayes (UVB), which begins with an application of VB at time $T_1$, resulting in the analytically tractable approximation to the posterior distribution,
\begin{equation}
\label{UVB:Time1}
q_{\lambda_1}(\theta | y_{1:T_1}) = \arg \underset{q}{\min} \hspace{1mm} KL[q_{\lambda_1} (\theta | y_{1:T_1}) \hspace{.1cm}||\hspace{.1cm}p(\theta | y_{1:T_1})].
\end{equation}
UVB then involves the recursive use of $q_{\lambda_n}(\theta | y_{1:T_n})$ at each time $T_n$ as a substitute for the true posterior in the Bayesian update equation (\ref{update:updatePost}) to form an alternative pseudo-posterior update,
\begin{equation}
\label{UVB:pHatPosterior}
\hat{p}(\theta |  y_{1:T_{n+1}}) \propto p(y_{T_{n}+1:T_{n+1}} | \theta)q_{\lambda_{n}}.(\theta | y_{1:T_{n}}).
\end{equation}
As the right hand side of this update is analytically tractable, standard ELBO gradient optimisation methods discussed in Section \ref{subsec:SVB} may be applied to find  
\begin{equation}
\label{UVB:TimeNp1}
q_{\lambda_{n+1}}(\theta | y_{1:T_{n+1}}) = \arg \underset{q}{\min} \hspace{1mm} KL[q_{\lambda_{n+1}} (\theta | y_{1:T_{n+1}}) \hspace{.1cm}||\hspace{.1cm}\hat{p}(\theta | y_{1:T_{n+1}})].
\end{equation}
Minimising the KL divergence in this way is not equivalent to standard VB which minimises $KL[q_{\lambda_{n+1}} (\theta | y_{1:T_{n+1}}) \hspace{.1cm}||\hspace{.1cm}p(\theta | y_{1:T_{n+1}})]$ unless $q_{\lambda_{n}}(\theta |  y_{1:T_{n}}) = p(\theta |  y_{1:T_{n}})$ almost everywhere. This is only possible if the posterior is a member of the family $q$. When this is not the case UVB introduces an additional approximation error to Variational Bayes inference which depends largely on the ability of the family $q$ chosen to adequately approximate the true posterior distribution.
\\

If the updates are equally spaced such that $T_{n+1} - T_{n} = H$ for all $n$, then the computational complexity of UVB as $n$ increases is constant at $O(H)$. In contrast, the computational complexity of SVB is $O(T_{n})$ as is requires evaluation of the complete data likelihood, $p(y_{1:T_{n}} | \theta)$, at each $T_{n}$. Choosing the form of the approximating distribution to match the prior distribution allows UVB to be easily implemented, as the prior hyper-parameters are simply replaced with the optimised $\lambda_n$ values after each update.
\\

A summary of the UVB algorithm is given by Algorithm \ref{alg:UVB}.

\begin{algorithm}[H]
 \SetKwInOut{Input}{Input}
 \Input{Prior, Likelihood.}
 \KwResult{Approximating distribution at $T_N$.}
 Observe $y_{1:T_1}$\;
 Use (\ref{scoreDeriv}) to construct $q_{\lambda_1}(\theta | y_{1:T_1})$\;
 \For{$n \mbox{ in } 2, \ldots, N$}{
   Observe $y_{T_{n-1}+1:T_n}$\;
   Use $q_{\lambda_{n-1}}(\theta | y_{1:T_{n-1}})$ and (\ref{scoreDeriv}) to construct $q_{\lambda_n}(\theta | y_{1:T_n})$.
  }
 \caption{Updating Variational Bayes}
  \label{alg:UVB}
\end{algorithm}



\section{UVB Importance Sampling} \label{sec:IS}

The increasing approximation error introduced by UVB could be offset by an importance sampling correction at each $T_n$. Importance sampling allows an expectation with respect to the true posterior $p(\theta | y_{1:T_n})$ to be evaluated using samples drawn from some proposal distribution, such as the VB $q_{\lambda_n}(\theta | y_{1:T_n})$, See (citation) for an overview. The importance sampler reweights particles $\theta^{(1)}, \dots, \theta^{(M)}$ from $q_{\lambda_n}(\theta | y_{1:T_n})$ to form 
\begin{equation}
\label{IS:Approx}
p^*(\theta | y_{1:T_n}) = \sum_{i=1}^M w^{(i)}_{T_n} \delta(\theta^{(i)})
\end{equation}
with weights given by
\begin{align}
\hat{w}^{(i)}_{T_n} &= \frac{p(\theta^{(i)}, y_{1:T_n})}{q(\theta^{(i)} | y_{1:T_n})} \label{IS:Weights} \\
w^{(i)}_{T_n} &= \frac{\hat{w}^{(i)}_{T_n}}{\sum_{i=1}^M \hat{w}^{(i)}_{T_n}} \label{IS:WeightsNorm}
\end{align}
\\
Setting $M$ to be a large value can make the approximation error arbitarily small, so an imporance sampled VB approximation (VB-IS) or UVB approximation (UVB-IS) allows the user to trade compuational time with approximation accuracy.
\\

If intermediate values $y_{T_n+h}, h = 1, 2, \dots, T_{n+1} - T_{n} - 1$, are observed, the importance sample approximation can be updated in a Sequential Monte Carlo (SMC, \cite{Doucet2000}) fashion without the need of a new UVB approximation by updating the weights via
\begin{align}
\hat{w}^{(i)}_{T_n+h} &= \frac{p(\theta^{(i)}, y_{1:T_n+h})}{q(\theta^{(i)} | y_{1:T_n})} \nonumber \\
&= \frac{p(y_{T_n+h} | \theta^{(i)})p(\theta^{(i)}, y_{1:T_n+h-1)}}{q(\theta^{(i)} | y_{1:T_n})} \nonumber \\
&= p(y_{T_n+h} | \theta^{(i)}) \hat{w}^{(i)}_{T_n+h-1}, \label{IS:UpdateWeights}
\end{align}
and
\begin{equation}
\label{IS:UpdateWeightsNorm}
w^{(i)}_{T_n+h} = \frac{\hat{w}^{(i)}_{T_n+h}}{\sum_{i=1}^M \hat{w}^{(i)}_{T_n+h}}.
\end{equation}
to yield
\begin{equation}
\label{IS:ApproxUpdate}
p^*(\theta | y_{1:T_n+h}) = \sum_{i=1}^M w^{(i)}_{T_n+h} \delta(\theta^{(i)}).
\end{equation}

Repeated SMC updates often exhibit weight decay, where the values of each $w^{(i)}_{T_n+h}$ except one approaches zero, reducing the effective sample size of the approximation. This can be alleviated in the UVB setting by sampling a fresh set of particles from $q_{\lambda_n}(\theta | y_{1:T_n})$ when the distribution becomes available.

\iffalse
\section{Approximation Error}

We consider three approaches to posterior inference:
\begin{enumerate}
\item Posterior sampling with MCMC,
\item Approximate Posterior inference with VB,
\item Approximate Posterior inference with UVB,
\end{enumerate}

As the number of iterations increases, samples generated by MCMC converge to serially correlated samples from the true posterior distribution and we will consider MCMC as exact inference for the purpose of measuring VB and UVB approximation error in this paper.
\\

In this section we discuss several choices loss functions to measure this error. In the general case, the distance between the posterior distribution $p(\theta | y_{1:T_n})$ and approximation $q_{\lambda_n}(\theta |  y_{1:T_n})$ is of interest, however there may be cases where a model specific loss is of interest: the accuracy of an application of $\theta$ is more relevant than the accuracy of the posterior approximation itself. 


\subsection{Posterior Distance}

Each VB algorithm results in a density for $q(\theta | x)$, however we only have access to the true posterior $p(\theta | y_{1:T_n})$ through a set of MCMC samples. To measure the distance between the posterior and its variational approximation we consider the empirical first order Wasserstein distance, $W_1$, between thinned MCMC samples $\theta^{(i)}_p, i = 1, \dots, N$ and independently generated samples $\theta^{(j)}_q, j = 1, \dots, N$ from $q_{\lambda_n}(\theta |  y_{1:T_n})$.
\\

The Wasserstein distance is given by
\begin{equation}
\label{wasserstein}
W_1 = \underset{\boldsymbol{\omega} \in \Omega}{\min} \sum_{i=1}^N \sum_{j=1}^N \omega_{[ij]} d_{ij}
\end{equation}
where $d_{ij}$ denotes the euclidean distance between $\theta^{(i)}_p$ and $\theta^{(j)}_q$, while $\Omega$ denotes the set of all bijections from $\theta_p$ to $\theta_q$: each $\boldsymbol{\omega}$ is a matrix with one $1$ in each row and column and zeros in all other locations.
\\

This is also known as the Earthmover distance and is calculated with the R package `transport' \citep{transport} using the forward and reverse auction algorithm of \citet{Bertsekas1992}.

\subsection{Predictive Model Loss}

For a predictive model, where the aim is to predict the value of $y_{T_n + h}$ given observations of $y_{1:T_n}$, we consider loss functions based on the accuracy of the predictive distribution.

For example, consider the AR(p) model given by
\begin{equation}
y_t = \mu + \sum_{s=1}^p \phi_s (y_{t-s} - \mu) + e_t,
\end{equation}
where $e_t \sim N(0, \sigma^2)$, with the parameter vector
\begin{equation}
\theta = \{\sigma^2, \mu, \phi_1, \dots, \phi_p \}.
\end{equation}
\\

Given the posterior distribution $p(\theta | y_{1:T_n})$, the predictive distribution associated with $y_{T_n +h}$ is given by
\begin{equation}
\label{forecastDistIntro}
p(y_{T_n + h} | y_{1:T_n}) = \int_{\theta} p(y_{T_n + h} | y_{1:T_n}, \theta)p(\theta | y_{1:T_n})d\theta.
\end{equation}
The loss function associated with this value is given by $L(n, h)$ and is measured by the predictive logscore,
\begin{equation}
\label{loss:logscoreIntro}
L(n, h) = \log(p(y_{T_n + h} | y_{1:T_n}))
\end{equation}

\subsection{Classification Model Loss}

Many models aim to associate a class label $k_i = 1 , \dots, K$ to observations of each unit $i$. Consider the problem where $T_n$ observations are recorded for each of $M$ units from a mixture of normal distributions,
\begin{equation}
\label{mixNormalDGP}
y_{i, t} \sim \sum_{j=1}^K \pi_{j} N(\mu_j, \sigma^2_{j}).
\end{equation}
At each time period we aim to infer the posterior distribution of each mixture components parameters $\{\mu_j, \sigma_j, \pi_j | k = 1, \dots, K\}$ and predict the class labels of $y_{i, 1:T_n}$. Approximation error on the mixture components is measured by the Wasserstein Distance, while class label loss function is proportion of correct classifications.

\subsection{State-Space Model Loss}

Many models of interest can be expressed as a dynamic state-space, with
\begin{align}
y_t &\sim p(y_t | x_{t}, \theta) \label{measure} \\
x_t &\sim p(x_t | x_{t-1}, \theta) \label{transition} ,
\end{align}
where inference of $p(x_t, \theta | y_{1:t})$ is desired at each $t$. This problem is known as \textit{Bayesian filtering}, where the posterior distribution is recursively updated from $p(x_{t-1}, \theta | y_{1:t-1})$ to $p(x_t, \theta | y_{1:t})$ by
\begin{align}
p(x_t, \theta | y_{1:t-1}) &= \int p(x_t | x_{t-1}, \theta) p(x_{t-1}, \theta | y_{1:t-1})dx_{t-1} \label{marginalise} \\
p(x_t, \theta | y_{1:t}) &\propto p(y_t | x_t, \theta) p(x_t, \theta | y_{1:t-1}) \label{update}
\end{align}
for a given prior distribution $p(x_0, \theta)$. 
\\

For a limited class of models the marginalisation over $x_{t-1}$ in (\ref{marginalise}) and posterior in (\ref{update}) are both analytically tractable for all $t$, such as when both (\ref{measure}) and (\ref{transition}) are linear and Gaussian. Typically this is not the case and an approximation is required, such as the extended Kalman filter \citep{Anderson1979}, the unscented Kalman filter \citep{Wan2000}, or particle filters \citet{Arulampalam2002}. Variational Particle Filtering is introduced by \citet{Smidl2008}, in cases where a subset $x_{1, t}$ of $x_t = \{x_{1, t}, x_{2, t}\}$ can be analytically marginalised, and does not discuss simultaneous inference of the static variables $\theta$ which is allowed with UVB below.
\\

Replacing $p(x_{t-1}, \theta | y_{1:t-1})$ with the Variational Bayes approximation $q(x_{t-1} | y_{1:t-1}, \theta)q(\theta|y_{1:t-1})$ we obtain 
\begin{align}
\hat{p}(x_t, \theta | y_{1:t-1}) &= q(\theta | y_{1:t-1}) \int p(x_t | x_{t-1}, \theta) q(x_{t-1} | y_{1:t-1}, \theta)dx_{t-1} \label{marginaliseHat} \\
\hat{p}(x_t, \theta | y_{1:t}) &\propto p(y_t | x_t, \theta)\hat{p}(x_t, \theta | y_{1:t-1}). \label{updateHat}
\end{align}

If (\ref{transition}) is linear and Gaussian, and  $q(x_{t-1} | y_{1:t-1}, \theta)$ is Gaussian, (\ref{marginaliseHat}) can be marginalised analytically, allowing the Variational Bayes approximation at time $t$ as
\begin{equation}
\label{UVBfilter}
\hat{p}(x_t, \theta | y_{1:t}) \approx q(x_{t} | y_{1:t}, \theta)q(\theta|y_{1:t}).
\end{equation}
\\

Let $a_{t, \alpha}$ and $b_{t, \alpha}$ be the lower and upper bounds of the $\alpha\%$ highest probability interval for $x_t$, that is
\begin{equation}
\label{HPI}
\{a_{t, \alpha}, b_{t, \alpha}\} = \arg \underset{\{a, b\}}{\min}\mbox{ } b - a, \mbox{ where } \int_a^b p(x_t | x_{1:t-1}, \theta, y_{1:t})dx_t = \alpha.
\end{equation}
\\

We define the loss function as the $\alpha\%$ latent state coverage rate, 
\begin{equation}
\label{coverage}
L(n, \alpha) = \frac{\sum_{t=1}^{T_n} I(a_{t, \alpha} < x_t < b_{t, \alpha})}{T_n}
\end{equation}
where $I(\cdot)$ is the indicator function, which equals one if the argument is true and zero otherwise.
\fi

\section{Simulation Results}
\label{sec:UVBSim}

In this section the approximation error for repeated UVB applications is compared relative to both SVB inference and exact inference implemented by MCMC for two simple problems: Time Series Forecasting and Mixture Model Clustering.

\subsection{Time Series Forecasting}
\label{subsec:UVBTS}

500 datasets $y_{1:300}$ are simulated according to the following AR3 model,
\begin{equation}
\label{UVB:TSAR3}
y_t = \mu + \phi_1 (y_{t-1} - \mu) + \phi_2 (y_{t-2} - \mu) + \phi_3 (y_{t-3} - \mu) + e_t
\end{equation}
where $e_t \sim N(0, \sigma^2)$. 
The parameters are collected as 
\begin{equation}
\label{UVB:TStheta}
\theta = \{\log(\sigma^2), \mu, \phi_1, \phi_2, \phi_3 \},
\end{equation}
and inference is faciliated by the use of the vague prior
\begin{equation}
\label{UVB:TSprior}
\theta \sim N(\boldsymbol{0}, 10 \mathbb{I})
\end{equation}
where $\boldsymbol{0}$ denotes the zero vector and $\mathbb{I}$ denotes the identity matrix.
\\

In each simulation, $\mu$ and each $\phi$ are initially drawn from a $N(\boldsymbol{0}, \mathbb{I})$ distribution, accepting only draws where each $\phi$ lies in the AR3 stationary region. $\sigma^{-2}$ is simulated from a $G(5, 5)$ distribution.
\\

At each time $t = 100, 101, \ldots, 300$, the MCMC posterior distribution $p(\theta | y_{1:t})$ is simulated by 15000 draws from a Random Walk Metropolis Hastings MCMC algorithm, discarding the first 10000 as a burn in period.
\\

VB and UVB approximations, $q_{VB}(\theta | y_{1:T_n})$ and $q_{UVB}(\theta | y_{1:T_n})$ respectively, are fit at each time $T_n = 75 + 25n$ for $n = 1, 2, \ldots 9$. In the case of VB, each approximation is fit from randomised starting values, while with UVB each approximation at $n > 1$ uses the converged values $\lambda_{n-1}$ from the previous update as a starting point. In each case, the distributional family for $q$ is chosen to be a $K = 1, 2$, or $3$ component mixture of multivariate normal distributions.
\\

We further apply importance sampler corrections following Section \ref{sec:IS} for each $t = 100, 101, \ldots, 300$, these are referred to as VB-IS and UVB-IS respectively for VB and UVB approximations utilised as the initial proposal distributions.
\\

Given the posterior, or its approximation, the forecast distribution for $y_{T_n+h}$ is given by
\begin{equation}
\label{UVB:TSforecastDist}
p(y_{T_n + h} | y_{1:T_n}) = \int_{\theta} p(y_{T_n + h} | y_{1:T_n}, \theta)p(\theta | y_{1:T_n})d\theta,
\end{equation}
which may be evaluated by $M$ samples from the posterior,
\begin{equation}
\label{UVB:TSforecastDistApprox}
\hat{p}(y_{T_n + h} | y_{1:T_n}) \approx \frac{1}{M} \sum_{m=1}^M  p(y_{T_n + h} | y_{1:T_n}, \theta^{(m)}).
\end{equation}
The loss function associated with this value is given by $L(n, h)$ and is measured by the predictive logscore,
\begin{equation}
\label{UVB:TSlogscore}
L(n, h) = \log(p(y_{T_n + h} | y_{1:T_n})).
\end{equation}

We make one step ahead forecast distributions after each observation and each simulated dataset using exact MCMC inference and each approximation method: VB, UVB, VB-IS and UVB-IS, and plot the approximate logscore minus the exact logscore in the left panel of Figure (\ref{fig:UVBAR3Timing}). While each approximation has a lower logscore compared to exact inference, there is no significant error associated with the use of UVB instead of standard VB.
\\

The right panel displayed the mean runtime for a singular VB or VB-IS fit after observation of data up until time $T_n$, compared to the mean runtime for UVB and UVB-IS first fit to time $T_1$, and updated $n-1$ times to $T_n$, with a shaded 50\% interval. The cumulative runtime of UVB is greater than VB, however it benefits by being able to provide an approximating distribution from $T_1$ onwards, while VB must wait until all data is observed at time $T_n$ to begin optimisation. We find no significant difference between each approximation method in terms of logscore or elapsed time for $K = 1, 2 $ or $3$ component mixture distributions employed as approximations.   

\begin{figure}%
    \centering
    {{\includegraphics[width=7cm, height = 6cm]{figures/AR3ls} }}%
    \qquad
    {{\includegraphics[width=7cm, height = 6cm]{figures/AR3timing} }}%
    \caption{Left: Average difference between forecast logscores between approximate inferences and MCMC. Right: Average VB runtime for one approximation at time $T_n$ and average cumulative UVB runtime for each $T_1, T_2, \ldots, T_n$. There is little difference in each methods logscore, and running UVB $n$ times is slightly slower than running VB once at time $T_n$, however UVB can be ran as data arrives instead of waiting for all available data to be observed.}%
    \label{fig:UVBAR3Timing}%
\end{figure}

\subsection{Mixture Model Clustering}
\label{subsec:UVBMMC}

Consider the two component mixture normal model for $i = 1, \ldots N$ and $t = 1, \ldots, T_n$  augmenting each unit $i$ with an auxiliary variables $k_i$ such that
\begin{equation}
\label{UVB:MMCmixNormalDGP2}
y_{i, t} | k_i = j \sim  N(\mu_j, \sigma^2_{j}).
\end{equation}
with 
\begin{equation}
\label{UVB:MMCkPrior}
k_i | \pi \sim Bin(1, \pi).
\end{equation}

Collecting the parameters of interest as
\begin{equation}
\label{UVB:MMCtheta}
\theta = \{\log(\sigma^2_1), \log(\sigma^2_2), \mu_1, \mu_2 \},
\end{equation}
and using the vague priors
\begin{align}
\theta \sim N(\boldsymbol{0}, 10 \mathbb{I}), \\
\pi_1 \sim Beta(\alpha, \beta). \label{UVB:MMCpiPriorMix}
\end{align}
we consider posterior inference on $p(\theta | y_{1:N, 1:T_n})$ where $y_{1:N, 1:T_N} = \{y_i, 1:T_n | i = 1, 2, \dots, N\}$. This requires the marginalisation of the full posterior $p(\theta, \pi, k_{1:N} | y_{1:N, 1:T_n})$ by first noting (\ref{UVB:MMCkPrior}) and (\ref{UVB:MMCpiPriorMix}) implies that
\begin{equation}
\label{UVB:MMCkMarginalMix}
p(k_i = j) = \frac{\mathcal{B}(j + \alpha, \beta - j + 1)}{\mathcal{B}(\alpha, \beta)}
\end{equation}
where $\mathcal{B}(\cdot, \cdot)$ denotes the Beta function, and marginalising over each $k_i$,
\begin{equation}
\label{UVB:MMCMarginal}
p(\theta | y_{1:N, 1:T_n}) \propto p(\theta) \prod_{i=1}^N \left( \sum_{j=1}^2 p(y_{i, 1:T_n} | \theta, k_i = j) p(k_i = j) \right)
\end{equation}
This distribution is approximated by employing distributions $q_{\lambda}(\theta | y_{1:N, 1:T_n})$ as either a $K = 1, 2,$ or $3$ component mixture of multivariate normal distributions. 
\\

This posterior approximation can be updated i,f at each time $T_{n+1}$, the marginalisation in (\ref{UVB:MMCMarginal}) can be applied to form the psuedo-posterior
\begin{equation}
\label{UVB:MMCUpdate}
\hat{p}(\theta | y_{1:T_{n+1}}) \propto q_{\lambda_{n}}(\theta | y_{i, 1:T_{n}}) \prod_{i=1}^N \left( \sum_{j=1}^2 p(y_{i, T_n+1:T_{n+1}} | \theta, k_i = j) p(k_i = j | y_{1:T_n}) \right).
\end{equation}
This requires marginal posterior distributions $p(k_i | y_{i, 1:T_{n}})$, which we replace with the psuedo-marginal,
\begin{align}
\hat{p}(k_i | y_{i, 1:T_{n}}) &= \int_{\theta} p(k_i | y_{i, 1:T_{n}}, \theta)q_{\lambda_{n}}(\theta | y_{i, 1:T_{n}}) d\theta \nonumber \\
&\propto \int_{\theta} p(y_{i, 1:T_n} | \theta, k_i) p(k_i) q_{\lambda_{n}}(\theta | y_{i, 1:T_{n}}) d\theta \nonumber \\
&\approx \frac{1}{M} \sum_{m=1}^M p(y_{i, 1:T_n} | \theta^{(m)} , k_i) p(k_i). \label{UVB:MMCpkHat}
\end{align}
where $\theta^{(m)} \sim q_{\lambda_{n}}(\theta | y_{i, 1:T_{n}})$ for $m = 1, 2, \ldots, M$. This step happens once per UVB update, so use of the entire dataset in (\ref{UVB:MMCpkHat}) does not incur a large computational cost.
\\

This distribution is also used to predict classes for each unit as
\begin{equation}
\hat{k}_i = \arg \underset{j}{\max}\mbox{ } p(k_i = j | y_{i, 1:T_n}).
\end{equation}

The proportion of correct predictions is used to assess the performance of each inference procedure.
\\

500 datasets are simulated with $N = 50$ units of length $T = 100$. For each $T_n = 10n, n = 1, 2, \ldots, 10$, we infer the posterior distribution $p(\theta, k_1, \ldots, k_N | y_{1:N, 1:T_n})$ using each of MCMC, VB, UVB, VB-IS and UVB-IS. Data is simulated by drawing each $\mu$ from an $N(0, 0.25)$ distribution and each $\sigma^2$ from a $U(1, 2)$ distribution. Units are randomly allocated to each group with $50\%$ probability. 
\\

The results are displayed in Figure (\ref{fig:UVBMMCResults}), where in the left panel UVB outperforms VB in terms of correct cluster allocations relative to MCMC. In the right panel the mean runtime of $n$ UVB updates is compared relative to a single SVB, where UVB is significantly faster. The value of $K$ does not have a significant impact, and importance sampling does not improve results.

\begin{figure}%
    \centering
    {{\includegraphics[width=7cm, height = 6cm]{figures/mixNormScore} }}%
    \qquad
    {{\includegraphics[width=7cm, height = 6cm]{figures/mixNormTiming} }}%
    \caption{Left: Average difference between proportion of correct classifications between approximate inferences and MCMC. Right: Average SVB runtime for one approximation at time $T_n$ and average cumulative UVB runtime for each $T_1, T_2, \ldots, T_n$. In this scenario the large amount of data significantly slows SVB fits and degrades the gradient ascent optimisation relative to UVB.}%
    \label{fig:UVBMMCResults}%
\end{figure}


\section{Smart Meter Forecasting}

A popular area of application for forecasting methodology is in short term electricity load forecasting (STLF), where forecasts for up to a day ahead are required. This literature traditionally uses a regions aggregate load at a given point in time as the time series to be forecasted, but the recent proliferation in smart meter data has caused increased interest in per household level forecasts. 
\\

Smart meter data introduces multiple challenges to STLF: the time series dynamics are muted at the household level and replaced by a large amount of stochastic noise, and these households can display heterogeneous behaviours. Further, there are significant computational challenges associated with the number of smart meters to forecast often being in the thousands.
\\

(Citations) have found that clustering households into groups of similar behaviour, and creating a model for each behaviour type can help alleviate the difficulties in heterogeneity.
\\

Despite the prevalence of noise in the individual data, the models used for this data are largely similar to the aggregate load forecasting literature, where a mix of machine learning methods, such as neural networks and support vector machines have been used alongside arima and exponential smoothing time series models and semi-parametric spline models.
\\

Machine Learning models and semi-parametrics are supported by their strength in modelling non-linear relationships between load, lags of load, and exogeneous variables such as temperature, humidity, or other weather conditions and time of day/week/year effects. It is widely agreed that temperature effects improve forecasts, even with the added problem of forecasting the temperature. Details on temperature forecasts are rare, but BoM one day forecasts are used by Rob Hyndman regularly. In households with both air-conditioning and heating, the temperature-load relationship is V shaped with a minimum at 18.3 C.
\\

Models are typically used to provide point estimates evaluated by MAPE, but density forecasts are becoming more popular, with quantile regression, scenario simulation + residual bootstraps, and conditional kernel density estimation employed, evaluated by MAPE, CRSP, or visually comparing ex-ante and ex-post densities. Bayesian models are rare but not completely absent.
\\

Half hour frequency data is common (possibly the most common short term forecasting frequency), and it is observed that the time dynamics in electricity data depends strongly on the time of day, with a different parameter set employed per half hour period.
\\

Our model would differ from the previous literature by:
\begin{enumerate}
\item Being fully Bayesian (or, at least, approximately).
\item Employing online inference.
\item Clustering as part of the model without aggregation.
\item Considering a mixture of models.
\end{enumerate}

\subsection{Model}

Half hourly electricity load data is collected from $N = 200$ Smart Meters in London for the period from 01/12/2012 until 31/01/2014. For each household $i = 1, 2, \ldots, N$ and time period $t = 1, 2, \ldots, T$, the raw data $y_{i, t}^*$ is measured in kiloWatt Hours per half-hour period. This is non-negative and strongly skewed, so it is log transformed via
\begin{equation}
\label{elec:logY}
y_{i, t} = \log(y_{i, t}^* + 0.01)
\end{equation}
and modelled with the $K = 3$ component mixture model
\begin{equation}
\label{elec:electricityModel}
p(y_{i, t} | \theta, k_i, y_{i, 1:t-1}) = \sum_{j=1}^K I(k_{i} = j) p_{j}(y_{i, t} | \theta, y_{i, 1:(t-1)})
\end{equation}
where $I(\cdot)$ is the indicator function and $k_i = 1, 2, \ldots, K$ is a mixture component indicator.
\\

The mixture formulation allows the households to cluster into $K$ latent groups with similar dynamic behaviour, as the household electricity consumption may depend strongly on factors such as working and sleeping hours. Clustering smart meter data into groups of households with similar behaviour and then creating a model per group has been shown by (citations) to reduce forecast error by as much as 30\% compared to employing a single model. Typically this clustering is carried out prior to the modelling process however in this application both clustering and parameter estimation are incorporated into a single step with the mixture model (\ref{elec:electricityModel}).
\\

Each model $j = 1, 2, \ldots, K$ follows a double seasonal \\ ARIMAX $(3, 0, 0)(3, 0, 0)_{48}(1, 0, 0)_{336}$ where the two seasonal components correspond to $48$ observations per day and $336$ observations per week with per-halfhour-period parameters,, given by
\begin{equation}
\label{elec:dynamic}
(1 - \phi_{1, j}^{(h)}L - \phi_{2, j}^{(h)}L^2 - \phi_{3, j}^{(h)} L^3)(1 - \phi_{48, j}^{(h)}L^{48} - \phi_{96, j}^{(h)}L^{96} - \phi_{144, j}^{(h)}L^{144})(1 - \phi_{336, j}^{(h)}L^{336}) (y_{i, t} -  \beta^{\prime, {(h)}}_{j} x_{t}) = \epsilon_{i, t}
\end{equation}
where $L$ is the lag operator corresponding to $L^n y_{i, t} = y_{i, t- n}$, $(h)$ denotes the half-hour period of the day at time $t$. $x_{t}$ is a length $9$ vector with a elements corresponding to an intercept, $|Temp^{(c)}_{t} - 18.3|$, where $Temp^{(c)}_{t}$ is the temperature in degrees Celsius, six indicator variables for day of the week, and an indicator variable for public holidays while $\beta_{j}^{(h)}$ is corresponding coefficient vector for that half-hour. Finally $\epsilon_{i, t}$ is modelled with a $N(0, \sigma^{2, (h)}_{j})$ distribution.
\\

Let $\theta^{(h)}_j = \{\log(\sigma^{2, (h)}_{j}, \beta_{j, 1}^{(h)}, \beta_{j, 2}^{(h)}, \ldots, \beta_{j, 9}^{(h)}, \phi_{1, j}^{(h)}, \phi_{2, j}^{(h)}, \ldots, \phi_{336, j}^{(h)}\}$, the following random walk prior is applied to impose smoothness on the evolution of $\theta^{(h)}_j$ across the 48 half-hour periods each day,
\begin{align}
\label{elec:dynamicPrior}
\theta^{(1)}_j &\sim N(\boldsymbol{0}, \mathbb{I}), \\
\theta^{(h)}_j &\sim N(\theta^{(h-1)}, 0.1^2 \mathbb{I}) \mbox{ for } h = 2, 3, \ldots, 48.
\end{align}
where $\boldsymbol{0}$ is the zero vector and $\mathbb{I}$ is the identity matrix.
The $K \times 48$ $\theta^{(h)}_j$ vectors are collectively referred to as $\theta$.
\\

The per household indicator variables $k_i$ are modelled by
\begin{equation} 
\label{elec:kConditional}
k_i \sim Multinomial(\boldsymbol{\pi})
\end{equation}
where 
\begin{equation}
\label{elec:piPrior}
\boldsymbol{\pi} \sim Dir(\alpha_1, \alpha_2, \ldots, \alpha_K)
\end{equation}
This combination implies that
\begin{equation}
\label{elec:kMarginal}
p(k_i = j) = \frac{\Gamma(\sum_{l=1}^K \alpha_l)}{\Gamma(1 + \sum_{l=1}^K \alpha_l)} \frac{\Gamma(1 + \alpha_j)}{\Gamma(\alpha_j)}.
\end{equation}


Each $k_i$ can be marginalised out of $p(\theta, k_{1:N} | y_{1:N, 1:T_n})$ to form a lower dimensional marginal posterior $p(\theta | y_{1:N, 1:T_n})$ via
\begin{equation}
p(\theta | y_{1:N, 1:T_n}) \propto p(\theta)  \prod_{i=1}^N\prod_{t=\tau}^{T_n}\sum_{j=1}^K p(k_i = j)p_{j}(y_{i, t} | \theta, y_{i, 1:t-1})
\end{equation}

UVB is applied to approximate the posterior distribution $p(\theta | y_{1:N, 1:T_n})$ with a diagonal covariance multivariate Gaussian distribution $q_{\lambda_n}(\theta | y_{1:200, 1:T_n})$, where the time period from the start of the sample until $T_1$ composes of the 2928 half hourly observations for the two month period from 01/12/2012 to 31/01/2013. There are a further 365 updates composing of one day's data, so that $T_366$ is the end of the sample at 31/01/2014. At the end of every update, probabilistic forecasts of the next days electricity consumption at half hourly intervals for each household are made via
\begin{align}
p(y_{i, T_n+h} | y_{1:N, 1:T_n}) = \int_{\theta} \sum_{j=1}^K \sum_{m=0}^1 &p(y_{i, T_n+h} | y_{1:N, 1:T_n}, \theta, k_i = j, s_{i, T_n} = m) \nonumber \\
&\times p(s_{i, T_n} = m | \theta, y_{1:N, 1:T_n}, k_i = j) \nonumber \\
&\times p(k_i = j | \theta, y_{1:N, 1:T_n}) \nonumber \\
&\times q_{\lambda_n}(\theta | y_{1:N, 1:T_n}) d\theta 
\end{align}
for $h = 1, 2, \ldots, 48$.
\\

The model is compared to other popular models for this type of data (TBD) and the same model without clustering $(K = 1)$.

\bibliographystyle{asa}
\bibliography{references}





\end{document}